{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Events - The Event-driven Workflow Automation Framework \u00b6 What is Argo Events? \u00b6 Argo Events is an event-driven workflow automation framework for Kubernetes which helps you trigger K8s objects, Argo Workflows, Serverless workloads, etc. on events from variety of sources like webhook, s3, schedules, messaging queues, gcp pubsub, sns, sqs, etc. Features \u00b6 Supports events from 20+ event sources. Ability to customize business-level constraint logic for workflow automation. Manage everything from simple, linear, real-time to complex, multi-source events. Supports Kubernetes Objects, Argo Workflow, AWS Lambda, Serverless, etc. as triggers. CloudEvents compliant. Getting Started \u00b6 Follow these instruction to set up Argo Events. Documentation \u00b6 Concepts . Argo Events in action . Deep dive into Argo Events . Triggers \u00b6 Argo Workflows Standard K8s Objects HTTP Requests / Serverless Workloads (OpenFaas, Kubeless, KNative etc.) AWS Lambda NATS Messages Kafka Messages Slack Notifications Argo Rollouts Custom Trigger / Build Your Own Trigger Apache OpenWhisk Event Sources \u00b6 Argo-Events supports 20+ event sources. The complete list of event sources is available here . Who uses Argo Events? \u00b6 Organizations below are officially using Argo Events. Please send a PR with your organization name if you are using Argo Events. BioBox Analytics BlackRock Canva Fairwinds InsideBoard Intuit Viaduct Community Blogs and Presentations \u00b6 Automation of Everything - How To Combine Argo Events, Workflows & Pipelines, CD, and Rollouts Argo Events - Event-Based Dependency Manager for Kubernetes Automating Research Workflows at BlackRock Designing A Complete CI/CD Pipeline CI/CD Pipeline Using Argo Events, Workflows, and CD TGI Kubernetes with Joe Beda: CloudEvents and Argo Events","title":"Overview"},{"location":"#argo-events-the-event-driven-workflow-automation-framework","text":"","title":"Argo Events - The Event-driven Workflow Automation Framework"},{"location":"#what-is-argo-events","text":"Argo Events is an event-driven workflow automation framework for Kubernetes which helps you trigger K8s objects, Argo Workflows, Serverless workloads, etc. on events from variety of sources like webhook, s3, schedules, messaging queues, gcp pubsub, sns, sqs, etc.","title":"What is Argo Events?"},{"location":"#features","text":"Supports events from 20+ event sources. Ability to customize business-level constraint logic for workflow automation. Manage everything from simple, linear, real-time to complex, multi-source events. Supports Kubernetes Objects, Argo Workflow, AWS Lambda, Serverless, etc. as triggers. CloudEvents compliant.","title":"Features"},{"location":"#getting-started","text":"Follow these instruction to set up Argo Events.","title":"Getting Started"},{"location":"#documentation","text":"Concepts . Argo Events in action . Deep dive into Argo Events .","title":"Documentation"},{"location":"#triggers","text":"Argo Workflows Standard K8s Objects HTTP Requests / Serverless Workloads (OpenFaas, Kubeless, KNative etc.) AWS Lambda NATS Messages Kafka Messages Slack Notifications Argo Rollouts Custom Trigger / Build Your Own Trigger Apache OpenWhisk","title":"Triggers"},{"location":"#event-sources","text":"Argo-Events supports 20+ event sources. The complete list of event sources is available here .","title":"Event Sources"},{"location":"#who-uses-argo-events","text":"Organizations below are officially using Argo Events. Please send a PR with your organization name if you are using Argo Events. BioBox Analytics BlackRock Canva Fairwinds InsideBoard Intuit Viaduct","title":"Who uses Argo Events?"},{"location":"#community-blogs-and-presentations","text":"Automation of Everything - How To Combine Argo Events, Workflows & Pipelines, CD, and Rollouts Argo Events - Event-Based Dependency Manager for Kubernetes Automating Research Workflows at BlackRock Designing A Complete CI/CD Pipeline CI/CD Pipeline Using Argo Events, Workflows, and CD TGI Kubernetes with Joe Beda: CloudEvents and Argo Events","title":"Community Blogs and Presentations"},{"location":"FAQ/","text":"FAQs \u00b6 Q. How to get started with Argo Events? A . The recommended way to get started with Argo Events is: Read the basic concepts about EventBus , Sensor and Event Source . Install Argo Events as outlined here . Read the tutorials available here . Q. Can I deploy event-source and sensor in a namespace different than argo-events ? A . Yes. If you want to deploy the event-source in a different namespace than argo-events , please update the event-source definition with the desired namespace and service account. Make sure to grant the service account the necessary roles . Q. How to debug Argo-Events. A . Make sure you have installed everything as instructed here . Make sure you have the EventBus resource created within the namespace. The event-bus, event-source and sensor pods must be running. If you see any issue with the pods, check the logs for sensor-controller, event-source-controller and event-bus-controller. If event-source and sensor pods are running, but you are not receiving any events: Make sure you have configured the event source correctly. Check the event-source pod's containers logs. Note: You can set the environment variable DEBUG_LOG:true in any of the containers to output debug logs. Q. The event-source pod is receiving events but nothing happens. A . Check the sensor resource is deployed and a pod is running for the resource. If the sensor pod is running, check for Started to subscribe events for triggers in the logs. If the sensor has subscribed to the event-bus but is unable to create the trigger resource, please raise an issue on GitHub. Q. Helm chart installation does not work. A. The Helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the Helm chart is outdated, we encourage you to contribute to the argo-helm repository on GitHub. Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use the Minio event-source for AWS S3 notifications? A. No. The Minio event-source is exclusively for use only with Minio servers. If you want to trigger workloads on an AWS S3 bucket notification, set up the AWS SNS event-source. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this functionality is offered by the sensor event resolution circuitry. Please take a look at the Circuit and Switch tutorial. Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag . Always use the pinned version of the images. We will try to keep the latest tag in sync with the most recently released version. Q. Where can I find the event structure for a particular event-source? A. Please refer to this file to understand the structure of different types of events dispatched by the event-source pod.","title":"FAQs"},{"location":"FAQ/#faqs","text":"Q. How to get started with Argo Events? A . The recommended way to get started with Argo Events is: Read the basic concepts about EventBus , Sensor and Event Source . Install Argo Events as outlined here . Read the tutorials available here . Q. Can I deploy event-source and sensor in a namespace different than argo-events ? A . Yes. If you want to deploy the event-source in a different namespace than argo-events , please update the event-source definition with the desired namespace and service account. Make sure to grant the service account the necessary roles . Q. How to debug Argo-Events. A . Make sure you have installed everything as instructed here . Make sure you have the EventBus resource created within the namespace. The event-bus, event-source and sensor pods must be running. If you see any issue with the pods, check the logs for sensor-controller, event-source-controller and event-bus-controller. If event-source and sensor pods are running, but you are not receiving any events: Make sure you have configured the event source correctly. Check the event-source pod's containers logs. Note: You can set the environment variable DEBUG_LOG:true in any of the containers to output debug logs. Q. The event-source pod is receiving events but nothing happens. A . Check the sensor resource is deployed and a pod is running for the resource. If the sensor pod is running, check for Started to subscribe events for triggers in the logs. If the sensor has subscribed to the event-bus but is unable to create the trigger resource, please raise an issue on GitHub. Q. Helm chart installation does not work. A. The Helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the Helm chart is outdated, we encourage you to contribute to the argo-helm repository on GitHub. Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use the Minio event-source for AWS S3 notifications? A. No. The Minio event-source is exclusively for use only with Minio servers. If you want to trigger workloads on an AWS S3 bucket notification, set up the AWS SNS event-source. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this functionality is offered by the sensor event resolution circuitry. Please take a look at the Circuit and Switch tutorial. Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag . Always use the pinned version of the images. We will try to keep the latest tag in sync with the most recently released version. Q. Where can I find the event structure for a particular event-source? A. Please refer to this file to understand the structure of different types of events dispatched by the event-source pod.","title":"FAQs"},{"location":"developer_guide/","text":"Developer Guide \u00b6 Setup your DEV environment \u00b6 Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3. Requirements \u00b6 Golang 1.15 Docker Installation & Setup \u00b6 1. Get the project \u00b6 git clone git @github . com : argoproj / argo - events cd argo - events 2. Start Minikube and point Docker Client to Minikube's Docker Daemon \u00b6 minikube start eval $(minikube docker-env) 3. Build the project \u00b6 make all Changing Types \u00b6 If you're making a change to the pkg/apis package, please ensure you re-run following command for code regeneration. $ make codegen","title":"Developer Guide"},{"location":"developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"developer_guide/#setup-your-dev-environment","text":"Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3.","title":"Setup your DEV environment"},{"location":"developer_guide/#requirements","text":"Golang 1.15 Docker","title":"Requirements"},{"location":"developer_guide/#installation-setup","text":"","title":"Installation &amp; Setup"},{"location":"developer_guide/#1-get-the-project","text":"git clone git @github . com : argoproj / argo - events cd argo - events","title":"1. Get the project"},{"location":"developer_guide/#2-start-minikube-and-point-docker-client-to-minikubes-docker-daemon","text":"minikube start eval $(minikube docker-env)","title":"2. Start Minikube and point Docker Client to Minikube's Docker Daemon"},{"location":"developer_guide/#3-build-the-project","text":"make all","title":"3. Build the project"},{"location":"developer_guide/#changing-types","text":"If you're making a change to the pkg/apis package, please ensure you re-run following command for code regeneration. $ make codegen","title":"Changing Types"},{"location":"eventbus/","text":"EventBus \u00b6 v0.17.0 and after EventBus is a Kubernetes Custom Resource which is used for events transmission from EventSources to Sensors. Currently EventBus is backed by NATS Streaming , and it is open to support other technologies. EventBus is namespaced, an EventBus object is required in a namespace to make EventSource and Sensor work. The common pratice is to create an EventBus named default in the namespace. If you want to use a different name, or you want to have multiple EventBus in one namespace, you need to specifiy eventBusName in the spec of EventSource and Sensor correspondingly, so that they can find the right one. See EventSource spec and Sensor spec . NATS Streaming \u00b6 You can create a native NATS EventBus, or connect to an existing NATS Streaming service with exotic NATS EventBus. Native \u00b6 A simplest native NATS EventBus example: apiVersion : argoproj.io/v1alpha1 kind : EventBus metadata : name : default spec : nats : native : {} The example above brings up a NATS Streaming StatefulSet with 3 replicas in the namespace. The following example shows an EventBus with token auth strategy and persistent volumes. apiVersion : argoproj.io/v1alpha1 kind : EventBus metadata : name : default spec : nats : native : replicas : 3 # optional, defaults to 3, and requires minimal 3 auth : token # optional, default to none persistence : # optional storageClassName : standard accessMode : ReadWriteOnce volumeSize : 10Gi Properties \u00b6 Check here for the full spec of native . replicas - StatefulSet replicas, defaults to 3, and requires minimal 3. According to NATS Streaming doc , the size should probably be limited to 3 to 5, and odd number is recommended. auth - The strategy that clients connect to NATS Streaming service, none or token is currently supported, defaults to none . If token strategy is used, the system will generate a token and store it in K8s secrets (one for client, one for server), EventSource and Sensor PODs will automatically load the client secret and use it to connect to the EventBus. antiAffinity - Whether to create the StatefulSet PODs with anti-affinity rule. Deprecated in v1.3 , will be removed in v1.5 , use affinity instead. nodeSelector - Node selector for StatefulSet PODs. tolerations - Tolerations for the PODs. persistence - Whether to use a persistence volume for the data. securityContext - POD level security attributes and common container settings. maxAge - Max Age of existing messages, i.e. 72h , 4h35m , defaults to 72h . imagePullSecrets - Secrets used to pull images. serviceAccountName - In case your firm requires to use a service account other than default . priority - Priority of the StatefulSet PODs. priorityClassName - PriorityClassName of the StatefulSet PODs. affinity - Affinity settings for the StatefulSet PODs. A best effort and a hard requirement node anti-affinity config look like below, if you want to do AZ (Availablity Zone) anti-affinity, change the value of topologyKey from kubernetes.io/hostname to topology.kubernetes.io/zone . # Best effort affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - podAffinityTerm : labelSelector : matchLabels : controller : eventbus-controller eventbus-name : default topologyKey : kubernetes.io/hostname weight : 100 # Hard requirement affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : controller : eventbus-controller eventbus-name : default topologyKey : kubernetes.io/hostname More About Native NATS EventBus \u00b6 Messages limit is 1,000,000. Max age of messages is 72 hours, which means messages over 72 hours will be deleted automatically. It can be cutomized by setting spec.nats.native.maxAge , i.e. 240h . Max subscription number is 1000. Exotic \u00b6 To use an existing NATS Streaming service, follow the example below. apiVersion : argoproj.io/v1alpha1 kind : EventBus metadata : name : default spec : nats : exotic : url : nats://xxxxx:xxx clusterID : cluster-id auth : token accessSecret : name : my-secret-name key : secret-key More Information \u00b6 To view a finalized EventBus config: kubectl get eventbus default -o json | jq '.status.config' A sample result: { \"nats\" : { \"accessSecret\" : { \"key\" : \"client-auth\" , \"name\" : \"eventbus-default-client\" }, \"auth\" : \"token\" , \"clusterID\" : \"eventbus-default\" , \"url\" : \"nats://eventbus-default-stan-svc:4222\" } } All the events in a namespace are published to same channel/subject/topic named eventbus-{namespace} in the EventBus.","title":"EventBus"},{"location":"eventbus/#eventbus","text":"v0.17.0 and after EventBus is a Kubernetes Custom Resource which is used for events transmission from EventSources to Sensors. Currently EventBus is backed by NATS Streaming , and it is open to support other technologies. EventBus is namespaced, an EventBus object is required in a namespace to make EventSource and Sensor work. The common pratice is to create an EventBus named default in the namespace. If you want to use a different name, or you want to have multiple EventBus in one namespace, you need to specifiy eventBusName in the spec of EventSource and Sensor correspondingly, so that they can find the right one. See EventSource spec and Sensor spec .","title":"EventBus"},{"location":"eventbus/#nats-streaming","text":"You can create a native NATS EventBus, or connect to an existing NATS Streaming service with exotic NATS EventBus.","title":"NATS Streaming"},{"location":"eventbus/#native","text":"A simplest native NATS EventBus example: apiVersion : argoproj.io/v1alpha1 kind : EventBus metadata : name : default spec : nats : native : {} The example above brings up a NATS Streaming StatefulSet with 3 replicas in the namespace. The following example shows an EventBus with token auth strategy and persistent volumes. apiVersion : argoproj.io/v1alpha1 kind : EventBus metadata : name : default spec : nats : native : replicas : 3 # optional, defaults to 3, and requires minimal 3 auth : token # optional, default to none persistence : # optional storageClassName : standard accessMode : ReadWriteOnce volumeSize : 10Gi","title":"Native"},{"location":"eventbus/#properties","text":"Check here for the full spec of native . replicas - StatefulSet replicas, defaults to 3, and requires minimal 3. According to NATS Streaming doc , the size should probably be limited to 3 to 5, and odd number is recommended. auth - The strategy that clients connect to NATS Streaming service, none or token is currently supported, defaults to none . If token strategy is used, the system will generate a token and store it in K8s secrets (one for client, one for server), EventSource and Sensor PODs will automatically load the client secret and use it to connect to the EventBus. antiAffinity - Whether to create the StatefulSet PODs with anti-affinity rule. Deprecated in v1.3 , will be removed in v1.5 , use affinity instead. nodeSelector - Node selector for StatefulSet PODs. tolerations - Tolerations for the PODs. persistence - Whether to use a persistence volume for the data. securityContext - POD level security attributes and common container settings. maxAge - Max Age of existing messages, i.e. 72h , 4h35m , defaults to 72h . imagePullSecrets - Secrets used to pull images. serviceAccountName - In case your firm requires to use a service account other than default . priority - Priority of the StatefulSet PODs. priorityClassName - PriorityClassName of the StatefulSet PODs. affinity - Affinity settings for the StatefulSet PODs. A best effort and a hard requirement node anti-affinity config look like below, if you want to do AZ (Availablity Zone) anti-affinity, change the value of topologyKey from kubernetes.io/hostname to topology.kubernetes.io/zone . # Best effort affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - podAffinityTerm : labelSelector : matchLabels : controller : eventbus-controller eventbus-name : default topologyKey : kubernetes.io/hostname weight : 100 # Hard requirement affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : controller : eventbus-controller eventbus-name : default topologyKey : kubernetes.io/hostname","title":"Properties"},{"location":"eventbus/#more-about-native-nats-eventbus","text":"Messages limit is 1,000,000. Max age of messages is 72 hours, which means messages over 72 hours will be deleted automatically. It can be cutomized by setting spec.nats.native.maxAge , i.e. 240h . Max subscription number is 1000.","title":"More About Native NATS EventBus"},{"location":"eventbus/#exotic","text":"To use an existing NATS Streaming service, follow the example below. apiVersion : argoproj.io/v1alpha1 kind : EventBus metadata : name : default spec : nats : exotic : url : nats://xxxxx:xxx clusterID : cluster-id auth : token accessSecret : name : my-secret-name key : secret-key","title":"Exotic"},{"location":"eventbus/#more-information","text":"To view a finalized EventBus config: kubectl get eventbus default -o json | jq '.status.config' A sample result: { \"nats\" : { \"accessSecret\" : { \"key\" : \"client-auth\" , \"name\" : \"eventbus-default-client\" }, \"auth\" : \"token\" , \"clusterID\" : \"eventbus-default\" , \"url\" : \"nats://eventbus-default-stan-svc:4222\" } } All the events in a namespace are published to same channel/subject/topic named eventbus-{namespace} in the EventBus.","title":"More Information"},{"location":"installation/","text":"Installation \u00b6 Requirements \u00b6 Kubernetes cluster >=v1.11 Installed the kubectl command-line tool >v1.11.0 Using kubectl \u00b6 Cluster-wide Installation \u00b6 Create the namespace kubectl create namespace argo-events Deploy Argo Events, SA, ClusterRoles, Sensor Controller, EventBus Controller and EventSource Controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml # Install with a validating admission controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml NOTE: * On GKE , you may need to grant your account the ability to create new custom resource definitions and clusterroles kubectl create clusterrolebinding YOURNAME - cluster - admin - binding -- clusterrole = cluster - admin -- user = YOUREMAIL @gmail.com * On Openshift , make sure to grant `anyuid` scc to the service account . oc adm policy add - scc - to - user anyuid system : serviceaccount : argo - events : default Deploy the eventbus, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Namespace Installation \u00b6 Create the namespace kubectl create namespace argo-events Deploy Argo Events, SA, Roles, Sensor Controller, EventBus Controller and EventSource Controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/namespace-install.yaml NOTE: * On GKE , you may need to grant your account the ability to create new custom resource definitions kubectl create clusterrolebinding YOURNAME - cluster - admin - binding -- clusterrole = cluster - admin -- user = YOUREMAIL @gmail.com * On Openshift , make sure to grant `anyuid` scc to the service account . oc adm policy add - scc - to - user anyuid system : serviceaccount : argo - events : default Deploy the eventbus, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Using Kustomize \u00b6 Use either cluster-install , or cluster-install-with-extension , or namespace-install folder as your base for Kustomize. kustomization.yaml : bases: - github.com/argoproj/argo-events/manifests/cluster-install # OR - github.com/argoproj/argo-events/manifests/namespace-install Using Helm Chart \u00b6 Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Create namespace called argo-events. Add argoproj repository helm repo add argo https://argoproj.github.io/argo-helm The helm chart for argo-events is maintained solely by the community and hence the image version for controllers can go out of sync. Update the image version in values.yaml to v1.0.0. Install argo-events chart helm install argo-events argo/argo-events Migrate to v1.0.0 \u00b6 If you are looking to migrate Argo Events <0.16.0 to v1.0.0, please read the migration docs .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Kubernetes cluster >=v1.11 Installed the kubectl command-line tool >v1.11.0","title":"Requirements"},{"location":"installation/#using-kubectl","text":"","title":"Using kubectl"},{"location":"installation/#cluster-wide-installation","text":"Create the namespace kubectl create namespace argo-events Deploy Argo Events, SA, ClusterRoles, Sensor Controller, EventBus Controller and EventSource Controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml # Install with a validating admission controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml NOTE: * On GKE , you may need to grant your account the ability to create new custom resource definitions and clusterroles kubectl create clusterrolebinding YOURNAME - cluster - admin - binding -- clusterrole = cluster - admin -- user = YOUREMAIL @gmail.com * On Openshift , make sure to grant `anyuid` scc to the service account . oc adm policy add - scc - to - user anyuid system : serviceaccount : argo - events : default Deploy the eventbus, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml","title":"Cluster-wide Installation"},{"location":"installation/#namespace-installation","text":"Create the namespace kubectl create namespace argo-events Deploy Argo Events, SA, Roles, Sensor Controller, EventBus Controller and EventSource Controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/namespace-install.yaml NOTE: * On GKE , you may need to grant your account the ability to create new custom resource definitions kubectl create clusterrolebinding YOURNAME - cluster - admin - binding -- clusterrole = cluster - admin -- user = YOUREMAIL @gmail.com * On Openshift , make sure to grant `anyuid` scc to the service account . oc adm policy add - scc - to - user anyuid system : serviceaccount : argo - events : default Deploy the eventbus, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml","title":"Namespace Installation"},{"location":"installation/#using-kustomize","text":"Use either cluster-install , or cluster-install-with-extension , or namespace-install folder as your base for Kustomize. kustomization.yaml : bases: - github.com/argoproj/argo-events/manifests/cluster-install # OR - github.com/argoproj/argo-events/manifests/namespace-install","title":"Using Kustomize"},{"location":"installation/#using-helm-chart","text":"Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Create namespace called argo-events. Add argoproj repository helm repo add argo https://argoproj.github.io/argo-helm The helm chart for argo-events is maintained solely by the community and hence the image version for controllers can go out of sync. Update the image version in values.yaml to v1.0.0. Install argo-events chart helm install argo-events argo/argo-events","title":"Using Helm Chart"},{"location":"installation/#migrate-to-v100","text":"If you are looking to migrate Argo Events <0.16.0 to v1.0.0, please read the migration docs .","title":"Migrate to v1.0.0"},{"location":"managed-namespace/","text":"Managed Namespace \u00b6 You can install argo-events in either cluster scoped or namespace scoped configuration, accordingly you need to set up ClusterRole or normal Role for service account argo-events-sa . In namespace scope installation, you must run eventbus-controller , eventsource-controller and sensor-controller with --namespaced . If you would like to have the controllers watching a separated namespace, add --managed-namespace as well. For example: - args: - --namespaced - --managed-namespace - default","title":"Managed Namespace"},{"location":"managed-namespace/#managed-namespace","text":"You can install argo-events in either cluster scoped or namespace scoped configuration, accordingly you need to set up ClusterRole or normal Role for service account argo-events-sa . In namespace scope installation, you must run eventbus-controller , eventsource-controller and sensor-controller with --namespaced . If you would like to have the controllers watching a separated namespace, add --managed-namespace as well. For example: - args: - --namespaced - --managed-namespace - default","title":"Managed Namespace"},{"location":"metrics/","text":"Prometheus Metrics \u00b6 v1.3 and after User Metrics \u00b6 Each of generated EventSource, Sensor and EventBus PODs exposes an HTTP endpoint for its metrics, which include things like how many events were generated, how many actions were triggered, and so on. To let your Prometheus server discover those user metrics, add following to your configuration. - job_name: ' argo - events ' kubernetes_sd_configs: - role: pod selectors: - role: pod label: ' controller in ( eventsource - controller , sensor - controller , eventbus - controller )' relabel_configs: - source_labels: [ __meta_kubernetes_pod_label_eventbus_name , __meta_kubernetes_pod_label_controller ] action: replace regex: (. + ); eventbus - controller replacement: $ 1 target_label: ' eventbus_name ' - source_labels: [ __meta_kubernetes_namespace , __meta_kubernetes_pod_label_controller ] action: replace regex: (. + ); eventbus - controller replacement: $ 1 target_label: ' namespace ' - source_labels: [ __address__ , __meta_kubernetes_pod_label_controller ] action: drop regex: (. + ) : ( \\d222);eventbus-controller Also please make sure your Prometheus Service Account has the permission to do POD discovery. A sample ClusterRole like below needs to be added or merged, and grant it to your Service Account. apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : pod-discovery rules : - apiGroups : [ \"\" ] resources : - pods verbs : [ \"get\" , \"list\" , \"watch\" ] EventSource \u00b6 argo_events_event_service_running_total \u00b6 How many configured events in the EventSource object are actively running. argo_events_events_sent_total \u00b6 How many events have been sent successfully. argo_events_events_sent_failed_total \u00b6 How many events failed to send. argo_events_event_processing_duration_milliseconds \u00b6 Event processing duration (from getting the event to send it to EventBus) in milliseconds - TBD. Sensor \u00b6 argo_events_action_triggered_total \u00b6 How many actions have been triggered successfully. argo_events_action_failed_total \u00b6 How many actions failed. argo_events_action_duration_milliseconds \u00b6 Action triggering duration. EventBus \u00b6 For native NATS EventBus, check this link for the metrics explanation. Controller Metrics \u00b6 If you are interested in Argo Events controller metrics, add following to your Prometheus configuration. - job_name: ' argo - events - controllers ' kubernetes_sd_configs: - role: pod selectors: - role: pod label: ' app in ( eventsource - controller , sensor - controller , eventbus - controller )' relabel_configs: - source_labels: [ __address__ , __meta_kubernetes_pod_label_app ] action: replace regex: (. + );( eventsource - controller | sensor - controller | eventbus - controller ) replacement: $ 1 : 7777 target_label: ' __address__ ' - source_labels: [ __meta_kubernetes_namespace , __meta_kubernetes_pod_label_app ] action: replace regex: (. + );( eventsource - controller | sensor - controller | eventbus - controller ) replacement: $ 1 target_label: ' namespace ' Golden Signals \u00b6 Following metrics are considered as Golden Signals of monitoring your applictions running with Argo Events. Latency argo_events_event_processing_duration_milliseconds argo_events_action_duration_milliseconds Traffic argo_events_events_sent_total argo_events_action_triggered_total Errors argo_events_events_sent_failed_total argo_events_action_failed_total Saturation argo_events_event_service_running_total . Other Kubernetes metrics such as CPU or memory.","title":"Prometheus Metrics"},{"location":"metrics/#prometheus-metrics","text":"v1.3 and after","title":"Prometheus Metrics"},{"location":"metrics/#user-metrics","text":"Each of generated EventSource, Sensor and EventBus PODs exposes an HTTP endpoint for its metrics, which include things like how many events were generated, how many actions were triggered, and so on. To let your Prometheus server discover those user metrics, add following to your configuration. - job_name: ' argo - events ' kubernetes_sd_configs: - role: pod selectors: - role: pod label: ' controller in ( eventsource - controller , sensor - controller , eventbus - controller )' relabel_configs: - source_labels: [ __meta_kubernetes_pod_label_eventbus_name , __meta_kubernetes_pod_label_controller ] action: replace regex: (. + ); eventbus - controller replacement: $ 1 target_label: ' eventbus_name ' - source_labels: [ __meta_kubernetes_namespace , __meta_kubernetes_pod_label_controller ] action: replace regex: (. + ); eventbus - controller replacement: $ 1 target_label: ' namespace ' - source_labels: [ __address__ , __meta_kubernetes_pod_label_controller ] action: drop regex: (. + ) : ( \\d222);eventbus-controller Also please make sure your Prometheus Service Account has the permission to do POD discovery. A sample ClusterRole like below needs to be added or merged, and grant it to your Service Account. apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : pod-discovery rules : - apiGroups : [ \"\" ] resources : - pods verbs : [ \"get\" , \"list\" , \"watch\" ]","title":"User Metrics"},{"location":"metrics/#eventsource","text":"","title":"EventSource"},{"location":"metrics/#argo_events_event_service_running_total","text":"How many configured events in the EventSource object are actively running.","title":"argo_events_event_service_running_total"},{"location":"metrics/#argo_events_events_sent_total","text":"How many events have been sent successfully.","title":"argo_events_events_sent_total"},{"location":"metrics/#argo_events_events_sent_failed_total","text":"How many events failed to send.","title":"argo_events_events_sent_failed_total"},{"location":"metrics/#argo_events_event_processing_duration_milliseconds","text":"Event processing duration (from getting the event to send it to EventBus) in milliseconds - TBD.","title":"argo_events_event_processing_duration_milliseconds"},{"location":"metrics/#sensor","text":"","title":"Sensor"},{"location":"metrics/#argo_events_action_triggered_total","text":"How many actions have been triggered successfully.","title":"argo_events_action_triggered_total"},{"location":"metrics/#argo_events_action_failed_total","text":"How many actions failed.","title":"argo_events_action_failed_total"},{"location":"metrics/#argo_events_action_duration_milliseconds","text":"Action triggering duration.","title":"argo_events_action_duration_milliseconds"},{"location":"metrics/#eventbus","text":"For native NATS EventBus, check this link for the metrics explanation.","title":"EventBus"},{"location":"metrics/#controller-metrics","text":"If you are interested in Argo Events controller metrics, add following to your Prometheus configuration. - job_name: ' argo - events - controllers ' kubernetes_sd_configs: - role: pod selectors: - role: pod label: ' app in ( eventsource - controller , sensor - controller , eventbus - controller )' relabel_configs: - source_labels: [ __address__ , __meta_kubernetes_pod_label_app ] action: replace regex: (. + );( eventsource - controller | sensor - controller | eventbus - controller ) replacement: $ 1 : 7777 target_label: ' __address__ ' - source_labels: [ __meta_kubernetes_namespace , __meta_kubernetes_pod_label_app ] action: replace regex: (. + );( eventsource - controller | sensor - controller | eventbus - controller ) replacement: $ 1 target_label: ' namespace '","title":"Controller Metrics"},{"location":"metrics/#golden-signals","text":"Following metrics are considered as Golden Signals of monitoring your applictions running with Argo Events. Latency argo_events_event_processing_duration_milliseconds argo_events_action_duration_milliseconds Traffic argo_events_events_sent_total argo_events_action_triggered_total Errors argo_events_events_sent_failed_total argo_events_action_failed_total Saturation argo_events_event_service_running_total . Other Kubernetes metrics such as CPU or memory.","title":"Golden Signals"},{"location":"quick_start/","text":"Getting Started \u00b6 We are going to set up a sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed to make this work. Make sure to have the eventbus pods running in the namespace. Run following command to create the eventbus, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Setup event-source for webhook as follows, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml The above event-source contains a single event configuration that runs an HTTP server on port 12000 with endpoint example . After running the above command, the event-source controller will create a pod and service. Create webhook sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml Once the sensor object is created, sensor controller will create corresponding pod and a service. Expose the event-source pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl -n argo-events port-forward $(kubectl -n argo-events get pod -l eventsource-name=webhook -o name) 12000:12000 & Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Verify that an Argo workflow was triggered. kubectl -n argo-events get workflows | grep \"webhook\"","title":"Getting Started"},{"location":"quick_start/#getting-started","text":"We are going to set up a sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed to make this work. Make sure to have the eventbus pods running in the namespace. Run following command to create the eventbus, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Setup event-source for webhook as follows, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml The above event-source contains a single event configuration that runs an HTTP server on port 12000 with endpoint example . After running the above command, the event-source controller will create a pod and service. Create webhook sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml Once the sensor object is created, sensor controller will create corresponding pod and a service. Expose the event-source pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl -n argo-events port-forward $(kubectl -n argo-events get pod -l eventsource-name=webhook -o name) 12000:12000 & Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Verify that an Argo workflow was triggered. kubectl -n argo-events get workflows | grep \"webhook\"","title":"Getting Started"},{"location":"service-accounts/","text":"Service Accounts \u00b6 Service Account for EventSources \u00b6 A Service Account can be specified in the EventSource object with spec.template.serviceAccountName , however it is not needed for all the EventSource types except resource . For a resource EventSource, you need to specify a Service Accout and give it list and watch permissions for the resource being watched. For example, if you want to watch actions on Deployment objects, you need to: Create a Service Account kubectl -n your-namespace create sa my-sa Grant RBAC privileges to it kubectl -n your-namespace create role deployments-watcher --verb=list,watch --resource=deployments.apps kubectl -n your-namespace create rolebinding deployments-watcher-role-binding --role=deployments-watcher --serviceaccount=your-namespace:my-sa or (if you want to watch cluster scope) kubectl create clusterrole deployments-watcher --verb=list,watch --resource=deployments.apps kubectl create clusterrolebinding deployments-watcher-clusterrole-binding --clusterrole=deployments-watcher --serviceaccount=your-namespace:my-sa Service Account for Sensors \u00b6 A Service Account also can be specified in a Sensor object via spec.template.serviceAccountName , this is only needed when k8s trigger or argoWorkflow trigger is defined in the Sensor object. The sensor examples provided by us use argo-events-sa service account to execute the triggers, but it has more permissions than needed, and you may want to limit those privileges based on your use-case. It's always a good practice to create a service account with minimum privileges to execute it. Argo Workflow Trigger \u00b6 To submit a workflow through argoWorkflow trigger, make sure to grant the Service Account create and list access to workflows.argoproj.io . To resubmit , retry , resume or suspend a workflow through argoWorkflow trigger, the service account needs update and get access to workflows.argoproj.io . K8s Resource Trigger \u00b6 To trigger a K8s resource including workflows.argoproj.io through k8s trigger, make sure to grant create permission to that resource. AWS Lambda, HTTP, Slack, NATS, Kafka, and OpenWhisk Triggers \u00b6 For these triggers, you don't need to specify a Service Account to the Sensor. Service Account for Trigged Workflows (or other K8s resources) \u00b6 When the Sensor is used to trigger a Workflow, you might need to configure the Service Account used in the Workflow spec ( NOT spec.template.serviceAccountName ) following Argo Workflow instructions . If it is used to trigger other K8s resources (i.e. a Deployment), make sure to follow least privilege principle.","title":"Service Accounts"},{"location":"service-accounts/#service-accounts","text":"","title":"Service Accounts"},{"location":"service-accounts/#service-account-for-eventsources","text":"A Service Account can be specified in the EventSource object with spec.template.serviceAccountName , however it is not needed for all the EventSource types except resource . For a resource EventSource, you need to specify a Service Accout and give it list and watch permissions for the resource being watched. For example, if you want to watch actions on Deployment objects, you need to: Create a Service Account kubectl -n your-namespace create sa my-sa Grant RBAC privileges to it kubectl -n your-namespace create role deployments-watcher --verb=list,watch --resource=deployments.apps kubectl -n your-namespace create rolebinding deployments-watcher-role-binding --role=deployments-watcher --serviceaccount=your-namespace:my-sa or (if you want to watch cluster scope) kubectl create clusterrole deployments-watcher --verb=list,watch --resource=deployments.apps kubectl create clusterrolebinding deployments-watcher-clusterrole-binding --clusterrole=deployments-watcher --serviceaccount=your-namespace:my-sa","title":"Service Account for EventSources"},{"location":"service-accounts/#service-account-for-sensors","text":"A Service Account also can be specified in a Sensor object via spec.template.serviceAccountName , this is only needed when k8s trigger or argoWorkflow trigger is defined in the Sensor object. The sensor examples provided by us use argo-events-sa service account to execute the triggers, but it has more permissions than needed, and you may want to limit those privileges based on your use-case. It's always a good practice to create a service account with minimum privileges to execute it.","title":"Service Account for Sensors"},{"location":"service-accounts/#argo-workflow-trigger","text":"To submit a workflow through argoWorkflow trigger, make sure to grant the Service Account create and list access to workflows.argoproj.io . To resubmit , retry , resume or suspend a workflow through argoWorkflow trigger, the service account needs update and get access to workflows.argoproj.io .","title":"Argo Workflow Trigger"},{"location":"service-accounts/#k8s-resource-trigger","text":"To trigger a K8s resource including workflows.argoproj.io through k8s trigger, make sure to grant create permission to that resource.","title":"K8s Resource Trigger"},{"location":"service-accounts/#aws-lambda-http-slack-nats-kafka-and-openwhisk-triggers","text":"For these triggers, you don't need to specify a Service Account to the Sensor.","title":"AWS Lambda, HTTP, Slack, NATS, Kafka, and OpenWhisk Triggers"},{"location":"service-accounts/#service-account-for-trigged-workflows-or-other-k8s-resources","text":"When the Sensor is used to trigger a Workflow, you might need to configure the Service Account used in the Workflow spec ( NOT spec.template.serviceAccountName ) following Argo Workflow instructions . If it is used to trigger other K8s resources (i.e. a Deployment), make sure to follow least privilege principle.","title":"Service Account for Trigged Workflows (or other K8s resources)"},{"location":"trigger-conditions/","text":"Trigger Conditions \u00b6 v1.0 and after Conditions is a new feature to replace Circuit and Switch . With conditions , triggers can be executed based on different dependency conditions. An example with conditions : apiVersion : argoproj.io/v1alpha1 kind : Sensor metadata : name : example spec : dependencies : - name : dep01 eventSourceName : webhook-a eventName : example01 - name : dep02 eventSourceName : webhook-a eventName : example02 - name : dep03 eventSourceName : webhook-b eventName : example03 triggers : - template : conditions : \"dep02\" name : trigger01 http : url : http://abc.com/hello1 method : GET - template : conditions : \"dep02 && dep03\" name : trigger02 http : url : http://abc.com/hello2 method : GET - template : conditions : \"(dep01 || dep02) && dep03\" name : trigger03 http : url : http://abc.com/hello3 method : GET Conditions is a boolean expression contains dependency names, the trigger won't be executed until the expression resolves to true. The operators in conditions include: && || Triggers Without Conditions \u00b6 If conditions is missing, the default conditions to execute the trigger is && logic of all the defined dependencies.","title":"Trigger Conditions"},{"location":"trigger-conditions/#trigger-conditions","text":"v1.0 and after Conditions is a new feature to replace Circuit and Switch . With conditions , triggers can be executed based on different dependency conditions. An example with conditions : apiVersion : argoproj.io/v1alpha1 kind : Sensor metadata : name : example spec : dependencies : - name : dep01 eventSourceName : webhook-a eventName : example01 - name : dep02 eventSourceName : webhook-a eventName : example02 - name : dep03 eventSourceName : webhook-b eventName : example03 triggers : - template : conditions : \"dep02\" name : trigger01 http : url : http://abc.com/hello1 method : GET - template : conditions : \"dep02 && dep03\" name : trigger02 http : url : http://abc.com/hello2 method : GET - template : conditions : \"(dep01 || dep02) && dep03\" name : trigger03 http : url : http://abc.com/hello3 method : GET Conditions is a boolean expression contains dependency names, the trigger won't be executed until the expression resolves to true. The operators in conditions include: && ||","title":"Trigger Conditions"},{"location":"trigger-conditions/#triggers-without-conditions","text":"If conditions is missing, the default conditions to execute the trigger is && logic of all the defined dependencies.","title":"Triggers Without Conditions"},{"location":"validating-admission-webhook/","text":"Validating Admission Webhook \u00b6 v1.3 and after Overview \u00b6 Starting from v1.3, a Validating Admission Webhook is introduced to the project. To install the validating webhook, use following command (change the version): kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/ { version } /manifests/install-validating-webhook.yaml Benefits \u00b6 Using the validating webhook has following benefits: It notifies the error at the time applying the faulty spec, so that you don't need to check the CRD object status field to see if there's any condition errors later on. e.g. Creating an exotic NATS EventBus without ClusterID specified: cat <<EOF | kubectl create -f - > apiVersion: argoproj.io/v1alpha1 > kind: EventBus > metadata: > name: default > spec: > nats: > exotic: {} > EOF Error from server ( BadRequest ) : error when creating \"STDIN\" : admission webhook \"webhook.argo-events.argoproj.io\" denied the request: \"spec.nats.exotic.clusterID\" is missing Spec updating behavior can be validated. Updating existing specs requires more validation, besides checking if the new spec is valid, we also need to check if there's any immutable fields being updated. This can not be done in the controller reconciliation, but we can do it by using the validating webhook. For example, updating Auth Strategy for a native NATS EventBus is prohibited, a denied response as following will be returned. Error from server ( BadRequest ) : error when applying patch: { \"metadata\" : { \"annotations\" : { \"kubectl.kubernetes.io/last-applied-configuration\" : \"{\\\"apiVersion\\\":\\\"argoproj.io/v1alpha1\\\",\\\"kind\\\":\\\"EventBus\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"default\\\",\\\"namespace\\\":\\\"argo-events\\\"},\\\"spec\\\":{\\\"nats\\\":{\\\"native\\\":{\\\"replicas\\\":3}}}}\\n\" }} , \"spec\" : { \"nats\" : { \"native\" : { \"auth\" :null, \"maxAge\" :null, \"securityContext\" :null }}}} to: Resource: \"argoproj.io/v1alpha1, Resource=eventbus\" , GroupVersionKind: \"argoproj.io/v1alpha1, Kind=EventBus\" Name: \"default\" , Namespace: \"argo-events\" for : \"test-eventbus.yaml\" : admission webhook \"webhook.argo-events.argoproj.io\" denied the request: \"spec.nats.native.auth\" is immutable, can not be updated","title":"Validating Admission Webhook"},{"location":"validating-admission-webhook/#validating-admission-webhook","text":"v1.3 and after","title":"Validating Admission Webhook"},{"location":"validating-admission-webhook/#overview","text":"Starting from v1.3, a Validating Admission Webhook is introduced to the project. To install the validating webhook, use following command (change the version): kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/ { version } /manifests/install-validating-webhook.yaml","title":"Overview"},{"location":"validating-admission-webhook/#benefits","text":"Using the validating webhook has following benefits: It notifies the error at the time applying the faulty spec, so that you don't need to check the CRD object status field to see if there's any condition errors later on. e.g. Creating an exotic NATS EventBus without ClusterID specified: cat <<EOF | kubectl create -f - > apiVersion: argoproj.io/v1alpha1 > kind: EventBus > metadata: > name: default > spec: > nats: > exotic: {} > EOF Error from server ( BadRequest ) : error when creating \"STDIN\" : admission webhook \"webhook.argo-events.argoproj.io\" denied the request: \"spec.nats.exotic.clusterID\" is missing Spec updating behavior can be validated. Updating existing specs requires more validation, besides checking if the new spec is valid, we also need to check if there's any immutable fields being updated. This can not be done in the controller reconciliation, but we can do it by using the validating webhook. For example, updating Auth Strategy for a native NATS EventBus is prohibited, a denied response as following will be returned. Error from server ( BadRequest ) : error when applying patch: { \"metadata\" : { \"annotations\" : { \"kubectl.kubernetes.io/last-applied-configuration\" : \"{\\\"apiVersion\\\":\\\"argoproj.io/v1alpha1\\\",\\\"kind\\\":\\\"EventBus\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"default\\\",\\\"namespace\\\":\\\"argo-events\\\"},\\\"spec\\\":{\\\"nats\\\":{\\\"native\\\":{\\\"replicas\\\":3}}}}\\n\" }} , \"spec\" : { \"nats\" : { \"native\" : { \"auth\" :null, \"maxAge\" :null, \"securityContext\" :null }}}} to: Resource: \"argoproj.io/v1alpha1, Resource=eventbus\" , GroupVersionKind: \"argoproj.io/v1alpha1, Kind=EventBus\" Name: \"default\" , Namespace: \"argo-events\" for : \"test-eventbus.yaml\" : admission webhook \"webhook.argo-events.argoproj.io\" denied the request: \"spec.nats.native.auth\" is immutable, can not be updated","title":"Benefits"},{"location":"webhook-authentication/","text":"Webhook Authentication \u00b6 v1.0 and after For webhook event source, if you want to get your endpoint protected from unauthorized accessing, you can specify authSecret to the spec, which is a K8s secret key selector. This simple authentication approach also works for webhook extended event sources, if that event source does not have a built in authenticator. Firstly, create a k8s secret containing your token. echo -n 'af3qqs321f2ddwf1e2e67dfda3fs' > ./token.txt kubectl create secret generic my-webhook-token --from-file = my-token = ./token.txt Then add authSecret to your webhook EventSource. apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : webhook : example : port : \"12000\" endpoint : /example method : POST authSecret : name : my-webhook-token key : my-token Now you can authenticate your webhook endpoint with the configured token. TOKEN = \"Bearer af3qqs321f2ddwf1e2e67dfda3fs\" curl -X POST -H \"Authorization: $TOKEN \" -d \"{your data}\" http://xxxxx:12000/example","title":"Webhook Authentication"},{"location":"webhook-authentication/#webhook-authentication","text":"v1.0 and after For webhook event source, if you want to get your endpoint protected from unauthorized accessing, you can specify authSecret to the spec, which is a K8s secret key selector. This simple authentication approach also works for webhook extended event sources, if that event source does not have a built in authenticator. Firstly, create a k8s secret containing your token. echo -n 'af3qqs321f2ddwf1e2e67dfda3fs' > ./token.txt kubectl create secret generic my-webhook-token --from-file = my-token = ./token.txt Then add authSecret to your webhook EventSource. apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : webhook : example : port : \"12000\" endpoint : /example method : POST authSecret : name : my-webhook-token key : my-token Now you can authenticate your webhook endpoint with the configured token. TOKEN = \"Bearer af3qqs321f2ddwf1e2e67dfda3fs\" curl -X POST -H \"Authorization: $TOKEN \" -d \"{your data}\" http://xxxxx:12000/example","title":"Webhook Authentication"},{"location":"webhook-health-check/","text":"Webhook Health Check \u00b6 v1.0 and after For webhook or webhook extended event sources such as github , gitlab , sns , slack , Storage GRID and stripe , besides the endpoint configured in the spec, an extra endpoint :${port}/health will also be created, this is useful for LB or Ingress configuration for the event source, where usually a health check endpoint is required. For example, the following EventSource object will have 4 endpoints created, :12000/example1 , :12000/health , :13000/example2 and :13000/health . A HTTP GET request to the health endpoint returns a text OK with HTTP response code 200 . apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : webhook : example : port : \"12000\" endpoint : /example1 method : POST example-foo : port : \"13000\" endpoint : /example2 method : POST","title":"Webhook Health Check"},{"location":"webhook-health-check/#webhook-health-check","text":"v1.0 and after For webhook or webhook extended event sources such as github , gitlab , sns , slack , Storage GRID and stripe , besides the endpoint configured in the spec, an extra endpoint :${port}/health will also be created, this is useful for LB or Ingress configuration for the event source, where usually a health check endpoint is required. For example, the following EventSource object will have 4 endpoints created, :12000/example1 , :12000/health , :13000/example2 and :13000/health . A HTTP GET request to the health endpoint returns a text OK with HTTP response code 200 . apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : webhook : example : port : \"12000\" endpoint : /example1 method : POST example-foo : port : \"13000\" endpoint : /example2 method : POST","title":"Webhook Health Check"},{"location":"concepts/architecture/","text":"Architecture \u00b6 Main components of Argo Events are: Event Source Sensor Eventbus Trigger","title":"Architecture"},{"location":"concepts/architecture/#architecture","text":"Main components of Argo Events are: Event Source Sensor Eventbus Trigger","title":"Architecture"},{"location":"concepts/event_source/","text":"Event Source \u00b6 An EventSource defines the configurations required to consume events from external sources like AWS SNS, SQS, GCP PubSub, Webhooks, etc. It further transforms the events into the cloudevents and dispatches them over to the eventbus. Available event-sources: AMQP AWS SNS AWS SQS Azure Events Hub Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Specification \u00b6 The complete specification is available here . Examples \u00b6 Examples are located under examples/event-sources .","title":"Event Source"},{"location":"concepts/event_source/#event-source","text":"An EventSource defines the configurations required to consume events from external sources like AWS SNS, SQS, GCP PubSub, Webhooks, etc. It further transforms the events into the cloudevents and dispatches them over to the eventbus. Available event-sources: AMQP AWS SNS AWS SQS Azure Events Hub Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis","title":"Event Source"},{"location":"concepts/event_source/#specification","text":"The complete specification is available here .","title":"Specification"},{"location":"concepts/event_source/#examples","text":"Examples are located under examples/event-sources .","title":"Examples"},{"location":"concepts/eventbus/","text":"Eventbus \u00b6 The eventbus acts as the transport layer of Argo-Events by connecting the event-sources and sensors. Event-Sources publish the events while the sensors subscribe to the events to execute triggers. The current implementation of the eventbus is powered by NATS streaming.","title":"Eventbus"},{"location":"concepts/eventbus/#eventbus","text":"The eventbus acts as the transport layer of Argo-Events by connecting the event-sources and sensors. Event-Sources publish the events while the sensors subscribe to the events to execute triggers. The current implementation of the eventbus is powered by NATS streaming.","title":"Eventbus"},{"location":"concepts/sensor/","text":"Sensor \u00b6 Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events on the eventbus and acts as an event dependency manager to resolve and execute the triggers. Event dependency \u00b6 A dependency is an event the sensor is waiting to happen. Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/sensors .","title":"Sensor"},{"location":"concepts/sensor/#sensor","text":"Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events on the eventbus and acts as an event dependency manager to resolve and execute the triggers.","title":"Sensor"},{"location":"concepts/sensor/#event-dependency","text":"A dependency is an event the sensor is waiting to happen.","title":"Event dependency"},{"location":"concepts/sensor/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/sensor/#examples","text":"Examples are located under examples/sensors .","title":"Examples"},{"location":"concepts/trigger/","text":"Trigger \u00b6 A Trigger is the resource/workload executed by the sensor once the event dependencies are resolved. Trigger Types \u00b6 AWS Lambda Apache OpenWhisk Argo Rollouts Argo Workflows Custom - Build Your Own HTTP Requests - Serverless Workloads (OpenFaas, Kubeless, KNative etc.) Kafka Messages Log Message - for debugging NATS Messages Slack Notifications Create any Kubernetes Objects Log (for debugging event bus messages)","title":"Trigger"},{"location":"concepts/trigger/#trigger","text":"A Trigger is the resource/workload executed by the sensor once the event dependencies are resolved.","title":"Trigger"},{"location":"concepts/trigger/#trigger-types","text":"AWS Lambda Apache OpenWhisk Argo Rollouts Argo Workflows Custom - Build Your Own HTTP Requests - Serverless Workloads (OpenFaas, Kubeless, KNative etc.) Kafka Messages Log Message - for debugging NATS Messages Slack Notifications Create any Kubernetes Objects Log (for debugging event bus messages)","title":"Trigger Types"},{"location":"eventsources/catup/","text":"Calender EventSource - Catchup \u00b6 Catchup feature allow Calender eventsources to execute the missed schedules from last run. Enable Catchup forEventSource Definition \u00b6 User can configure catchup on each events in eventsource. example-with-catchup : # Catchup the missed events from last Event timestamp. last event will be persisted in configmap. schedule : \"* * * * *\" persistence : catchup : enabled : true # Check missed schedules from last persisted event time on Every start maxDuration : 5m # maximum amount of duration go back for the catchup configMap : # Configmap for persist the last successful event timestamp createIfNotExist : true name : test-configmap Last calender event persisted in configured configmap. Multiple event can use the same configmap to persist the events. data : calendar.example-with-catchup : '{\"eventTime\":\"2020-10-19 22:50:00.0003192 +0000 UTC m=+683.567066901\"}' Disable the catchup \u00b6 Set false to catchup-->enabled element catchup : enabled : false","title":"Calender EventSource - Catchup"},{"location":"eventsources/catup/#calender-eventsource-catchup","text":"Catchup feature allow Calender eventsources to execute the missed schedules from last run.","title":"Calender EventSource - Catchup"},{"location":"eventsources/catup/#enable-catchup-foreventsource-definition","text":"User can configure catchup on each events in eventsource. example-with-catchup : # Catchup the missed events from last Event timestamp. last event will be persisted in configmap. schedule : \"* * * * *\" persistence : catchup : enabled : true # Check missed schedules from last persisted event time on Every start maxDuration : 5m # maximum amount of duration go back for the catchup configMap : # Configmap for persist the last successful event timestamp createIfNotExist : true name : test-configmap Last calender event persisted in configured configmap. Multiple event can use the same configmap to persist the events. data : calendar.example-with-catchup : '{\"eventTime\":\"2020-10-19 22:50:00.0003192 +0000 UTC m=+683.567066901\"}'","title":"Enable Catchup forEventSource Definition"},{"location":"eventsources/catup/#disable-the-catchup","text":"Set false to catchup-->enabled element catchup : enabled : false","title":"Disable the catchup"},{"location":"eventsources/deployment-strategies/","text":"EventSource Deployment Strategies \u00b6 EventSource controller creates a k8s deployment for each EventSource object to watch the events. Some of the event source types do not allow multiple live clients with same attributes (i.e. multiple clients with same clientID connecting to a NATS server), or multiple event source PODs will generate duplicated events to downstream, so the deployment strategy and replica numbers are different for different event sources. Rolling Update Strategy \u00b6 Rolling Update strategy is applied to the following EventSource types: AWS SNS AWS SQS Github Gitlab NetApp Storage GRID Slack Stripe Webhook Replicas Of Rolling Update Types \u00b6 Deployment replica of these event source types respects spec.replica in the EventSource object, defaults to 1. Recreate Strategy \u00b6 Recreate strategy is applied to the following EventSource types: AMQP Azure Events Hub Kafka GCP PubSub File HDFS NATS Minio MQTT Emitter NSQ Pulsar Redis Resource Calendar Generic Replicas Of Recreate Types \u00b6 spec.replica in the Recreate types EventSources is ignored, the deployment is always created with replica=1 . Please DO NOT manually scale up the replicas, that will lead to unexpected behaviors!","title":"EventSource Deployment Strategies"},{"location":"eventsources/deployment-strategies/#eventsource-deployment-strategies","text":"EventSource controller creates a k8s deployment for each EventSource object to watch the events. Some of the event source types do not allow multiple live clients with same attributes (i.e. multiple clients with same clientID connecting to a NATS server), or multiple event source PODs will generate duplicated events to downstream, so the deployment strategy and replica numbers are different for different event sources.","title":"EventSource Deployment Strategies"},{"location":"eventsources/deployment-strategies/#rolling-update-strategy","text":"Rolling Update strategy is applied to the following EventSource types: AWS SNS AWS SQS Github Gitlab NetApp Storage GRID Slack Stripe Webhook","title":"Rolling Update Strategy"},{"location":"eventsources/deployment-strategies/#replicas-of-rolling-update-types","text":"Deployment replica of these event source types respects spec.replica in the EventSource object, defaults to 1.","title":"Replicas Of Rolling Update Types"},{"location":"eventsources/deployment-strategies/#recreate-strategy","text":"Recreate strategy is applied to the following EventSource types: AMQP Azure Events Hub Kafka GCP PubSub File HDFS NATS Minio MQTT Emitter NSQ Pulsar Redis Resource Calendar Generic","title":"Recreate Strategy"},{"location":"eventsources/deployment-strategies/#replicas-of-recreate-types","text":"spec.replica in the Recreate types EventSources is ignored, the deployment is always created with replica=1 . Please DO NOT manually scale up the replicas, that will lead to unexpected behaviors!","title":"Replicas Of Recreate Types"},{"location":"eventsources/gcp-pubsub/","text":"GCP PubSub \u00b6 Topic And Subscription ID \u00b6 GCP PubSub event source can listen to a PubSub with given topic , or subscriptionID . Here is the logic with different topic and subscriptionID combination. Topic Provided/Existing Sub ID Provided/Existing Actions Yes/Yes Yes/Yes Validate if given topic matches subsciption's topic Yes/Yes Yes/No Create a subscription with given ID Yes/Yes No/- Create or re-use subscription with auto generated subID Yes/No Yes/No Create a topic and a subscription with given subID Yes/No Yes/Yes Invalid Yes/No No/- Create a topic, create or re-use subscription w/ auto generated subID No/- Yes/Yes OK No/- Yes/No Invalid Workload Identity \u00b6 If you have configured Workload Identity and want to use it for a PubSub EventSource, leave credentialSecret nil. Full spec is available here . See a PubSub EventSource example .","title":"GCP PubSub"},{"location":"eventsources/gcp-pubsub/#gcp-pubsub","text":"","title":"GCP PubSub"},{"location":"eventsources/gcp-pubsub/#topic-and-subscription-id","text":"GCP PubSub event source can listen to a PubSub with given topic , or subscriptionID . Here is the logic with different topic and subscriptionID combination. Topic Provided/Existing Sub ID Provided/Existing Actions Yes/Yes Yes/Yes Validate if given topic matches subsciption's topic Yes/Yes Yes/No Create a subscription with given ID Yes/Yes No/- Create or re-use subscription with auto generated subID Yes/No Yes/No Create a topic and a subscription with given subID Yes/No Yes/Yes Invalid Yes/No No/- Create a topic, create or re-use subscription w/ auto generated subID No/- Yes/Yes OK No/- Yes/No Invalid","title":"Topic And Subscription ID"},{"location":"eventsources/gcp-pubsub/#workload-identity","text":"If you have configured Workload Identity and want to use it for a PubSub EventSource, leave credentialSecret nil. Full spec is available here . See a PubSub EventSource example .","title":"Workload Identity"},{"location":"eventsources/generic/","text":"Generic EventSource \u00b6 Generic eventsource extends Argo-Events eventsources via a simple gRPC contract. This is specifically useful when you want to onboard a custom eventsource implementation. Contract \u00b6 In order to qualify as generic eventsource, the eventsource server needs to implement following gRPC contract, syntax = \"proto3\" ; package generic ; service Eventing { rpc StartEventSource ( EventSource ) returns ( stream Event ); } message EventSource { // The event source name . string name = 1 ; // The event source configuration value . bytes config = 2 ; } /** * Represents an event */ message Event { // The event source name . string name = 1 ; // The event payload . bytes payload = 2 ; } The proto file is available here . Architecture \u00b6 Consider a generic eventsource, apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: generic spec: generic: example: insecure: true url: \"generic-event-source-server.argo-events.svc:8080\" config: |- key1: value1 key2: value2 The values placed under config field follows a free-form style and Argo-Events eventsource client is not opinionated about them. Although, it is expected that the eventsource server implemented by the user is able to parse the configuration. Flow \u00b6 The eventsource client connects to the server via the url defined under eventsource spec and sends over the configuration defined under config over an RPC call. The eventsource server then parses the configuration and connects to any external source if required to consume the events. The eventsource server can produce events without connecting to any external source, e.g. a special implementation of calendar events. The events from eventsource server are streamed back to the client. Client then writes the events to the eventbus which are read by the sensor to trigger the workflows. Connection Strategy \u00b6 The eventsource client performs indefinite retries to connect to the eventsource server and receives events over a stream upon successful connection. This also applies when the eventsource server goes down. Deployment Strategy \u00b6 The deployment strategy for the generic eventsource is Recreate . More info is available here .","title":"Generic EventSource"},{"location":"eventsources/generic/#generic-eventsource","text":"Generic eventsource extends Argo-Events eventsources via a simple gRPC contract. This is specifically useful when you want to onboard a custom eventsource implementation.","title":"Generic EventSource"},{"location":"eventsources/generic/#contract","text":"In order to qualify as generic eventsource, the eventsource server needs to implement following gRPC contract, syntax = \"proto3\" ; package generic ; service Eventing { rpc StartEventSource ( EventSource ) returns ( stream Event ); } message EventSource { // The event source name . string name = 1 ; // The event source configuration value . bytes config = 2 ; } /** * Represents an event */ message Event { // The event source name . string name = 1 ; // The event payload . bytes payload = 2 ; } The proto file is available here .","title":"Contract"},{"location":"eventsources/generic/#architecture","text":"Consider a generic eventsource, apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: generic spec: generic: example: insecure: true url: \"generic-event-source-server.argo-events.svc:8080\" config: |- key1: value1 key2: value2 The values placed under config field follows a free-form style and Argo-Events eventsource client is not opinionated about them. Although, it is expected that the eventsource server implemented by the user is able to parse the configuration.","title":"Architecture"},{"location":"eventsources/generic/#flow","text":"The eventsource client connects to the server via the url defined under eventsource spec and sends over the configuration defined under config over an RPC call. The eventsource server then parses the configuration and connects to any external source if required to consume the events. The eventsource server can produce events without connecting to any external source, e.g. a special implementation of calendar events. The events from eventsource server are streamed back to the client. Client then writes the events to the eventbus which are read by the sensor to trigger the workflows.","title":"Flow"},{"location":"eventsources/generic/#connection-strategy","text":"The eventsource client performs indefinite retries to connect to the eventsource server and receives events over a stream upon successful connection. This also applies when the eventsource server goes down.","title":"Connection Strategy"},{"location":"eventsources/generic/#deployment-strategy","text":"The deployment strategy for the generic eventsource is Recreate . More info is available here .","title":"Deployment Strategy"},{"location":"eventsources/multiple-events/","text":"EventSource With Multiple Events \u00b6 v0.17.0 and after Multiple events can be configured in a single EventSource, they can be either one event source type, or mixed event source types with some limitations. Single EventSource Type \u00b6 A single type EventSource configuration: apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : webhook : example : port : \"12000\" endpoint : /example method : POST example-foo : port : \"13000\" endpoint : /example2 method : POST For the example above, there are 2 events configured in the EventSource named webhook . Please use different port numbers for different events, this is the limitation for multiple events configured in a webhook EventSource, this limitation also applies to webhook extended event source types such as github , sns . Mixed EventSource Types \u00b6 EventSource is allowed to have mixed types of events configured. apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : mixed-sources spec : webhook : webhook-example : # eventName port : \"12000\" endpoint : /example method : POST sns : sns-example : # eventName topicArn : arn:aws:sns:us-east-1:XXXXXXXX:test webhook : endpoint : \"/\" port : \"15000\" accessKey : key : my-key name : my-name secretKey : key : my-secret-key name : my-secret-name region : us-east-1 However, there are some rules need to follow to do it: Rolling Update types and Recreate types can not be configured together, see EventSource Deployment Strategies . Event Name (i.e. webhook-example and sns-example above, refer to EventSource Names ) needs to be unique in the EventSource, same eventName is not allowed even they are in different event source types. The reason for that is, we use eventSourceName and eventName as the dependency attributes in Sensor.","title":"EventSource With Multiple Events"},{"location":"eventsources/multiple-events/#eventsource-with-multiple-events","text":"v0.17.0 and after Multiple events can be configured in a single EventSource, they can be either one event source type, or mixed event source types with some limitations.","title":"EventSource With Multiple Events"},{"location":"eventsources/multiple-events/#single-eventsource-type","text":"A single type EventSource configuration: apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : webhook : example : port : \"12000\" endpoint : /example method : POST example-foo : port : \"13000\" endpoint : /example2 method : POST For the example above, there are 2 events configured in the EventSource named webhook . Please use different port numbers for different events, this is the limitation for multiple events configured in a webhook EventSource, this limitation also applies to webhook extended event source types such as github , sns .","title":"Single EventSource Type"},{"location":"eventsources/multiple-events/#mixed-eventsource-types","text":"EventSource is allowed to have mixed types of events configured. apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : mixed-sources spec : webhook : webhook-example : # eventName port : \"12000\" endpoint : /example method : POST sns : sns-example : # eventName topicArn : arn:aws:sns:us-east-1:XXXXXXXX:test webhook : endpoint : \"/\" port : \"15000\" accessKey : key : my-key name : my-name secretKey : key : my-secret-key name : my-secret-name region : us-east-1 However, there are some rules need to follow to do it: Rolling Update types and Recreate types can not be configured together, see EventSource Deployment Strategies . Event Name (i.e. webhook-example and sns-example above, refer to EventSource Names ) needs to be unique in the EventSource, same eventName is not allowed even they are in different event source types. The reason for that is, we use eventSourceName and eventName as the dependency attributes in Sensor.","title":"Mixed EventSource Types"},{"location":"eventsources/naming/","text":"EventSource Names \u00b6 In a Sensor object, a dependency is defined as: dependencies : - name : test-dep eventSourceName : webhook-example eventName : example The eventSourceName and eventName might be confusing. Take the following EventSource example, the eventSourceName and eventName are described as below. apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook-example # eventSourceName spec : webhook : example : # eventName port : \"12000\" endpoint : /example method : POST example-foo : # eventName port : \"13000\" endpoint : /example2 method : POST EventSourceName \u00b6 eventSourceName is the name of the dependent EventSource object, i.e. webhook-example in the example above. EventName \u00b6 eventName is the map key of a configured event. In the example above, eventName could be exmaple or example-foo .","title":"EventSource Names"},{"location":"eventsources/naming/#eventsource-names","text":"In a Sensor object, a dependency is defined as: dependencies : - name : test-dep eventSourceName : webhook-example eventName : example The eventSourceName and eventName might be confusing. Take the following EventSource example, the eventSourceName and eventName are described as below. apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook-example # eventSourceName spec : webhook : example : # eventName port : \"12000\" endpoint : /example method : POST example-foo : # eventName port : \"13000\" endpoint : /example2 method : POST","title":"EventSource Names"},{"location":"eventsources/naming/#eventsourcename","text":"eventSourceName is the name of the dependent EventSource object, i.e. webhook-example in the example above.","title":"EventSourceName"},{"location":"eventsources/naming/#eventname","text":"eventName is the map key of a configured event. In the example above, eventName could be exmaple or example-foo .","title":"EventName"},{"location":"eventsources/services/","text":"EventSource Services \u00b6 Some of the EventSources ( webhook , github , gitlab , sns , slack , Storage GRID and stripe ) start an HTTP service to receive the events, for your convenience, there is a field named service within EventSource spec can help you create a ClusterIP service for testing. For example: apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : service : ports : - port : 12000 targetPort : 12000 webhook : example : port : \"12000\" endpoint : /example method : POST However, the generated service is ONLY for testing purpose, if you want to expose the endpoint for external access, please manage it by using native K8s objects (i.e. a Load Balancer type Service, or an Ingress), and remove service field from the EventSource object. You can refer to webhook heath check if you need a health check endpoint for LB Service or Ingress configuration.","title":"EventSource Services"},{"location":"eventsources/services/#eventsource-services","text":"Some of the EventSources ( webhook , github , gitlab , sns , slack , Storage GRID and stripe ) start an HTTP service to receive the events, for your convenience, there is a field named service within EventSource spec can help you create a ClusterIP service for testing. For example: apiVersion : argoproj.io/v1alpha1 kind : EventSource metadata : name : webhook spec : service : ports : - port : 12000 targetPort : 12000 webhook : example : port : \"12000\" endpoint : /example method : POST However, the generated service is ONLY for testing purpose, if you want to expose the endpoint for external access, please manage it by using native K8s objects (i.e. a Load Balancer type Service, or an Ingress), and remove service field from the EventSource object. You can refer to webhook heath check if you need a health check endpoint for LB Service or Ingress configuration.","title":"EventSource Services"},{"location":"setup/amqp/","text":"AMQP \u00b6 AMQP event-source listens to messages on the MQ and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"contentType\": \"ContentType is the MIME content type\", \"contentEncoding\": \"ContentEncoding is the MIME content encoding\", \"deliveryMode\": \"Delivery mode can be either - non-persistent (1) or persistent (2)\", \"priority\": \"Priority refers to the use - 0 to 9\", \"correlationId\": \"CorrelationId is the correlation identifier\", \"replyTo\": \"ReplyTo is the address to reply to (ex: RPC)\", \"expiration\": \"Expiration refers to message expiration spec\", \"messageId\": \"MessageId is message identifier\", \"timestamp\": \"Timestamp refers to the message timestamp\", \"type\": \"Type refers to the message type name\", \"appId\": \"AppId refers to the application id\", \"exchange\": \"Exchange is basic.publish exchange\", \"routingKey\": \"RoutingKey is basic.publish routing key\", \"body\": \"Body represents the messsage body\", } } Setup \u00b6 Lets set up RabbitMQ locally, apiVersion : v1 kind : Service metadata : labels : component : rabbitmq name : rabbitmq - service spec : ports : - port : 5672 selector : app : taskQueue component : rabbitmq --- apiVersion : v1 kind : ReplicationController metadata : labels : component : rabbitmq name : rabbitmq - controller spec : replicas : 1 template : metadata : labels : app : taskQueue component : rabbitmq spec : containers : - image : rabbitmq name : rabbitmq ports : - containerPort : 5672 resources : limits : cpu : 100 m Make sure the RabbitMQ controller pod is up and running before proceeding further. Expose the RabbitMQ server to local publisher using port-forward , kubectl -n argo-events port-forward <rabbitmq-pod-name> 5672:5672 Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/amqp.yaml Inspect the event-source pod logs to make sure it was able to subscribe to the exchange specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/amqp.yaml Lets set up a rabbitmq publisher. If you don't have pika installed, run, python -m pip install pika --upgrade Open a python REPL and run following code to publish a message on exhange called test . import pika connection = pika . BlockingConnection ( pika . ConnectionParameters ( 'localhost' )) channel = connection . channel () channel . basic_publish ( exchange = 'test' , routing_key = 'hello' , body = '{\"message\": \"hello\"}' ) As soon as you publish a message, sensor will trigger an Argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AMQP"},{"location":"setup/amqp/#amqp","text":"AMQP event-source listens to messages on the MQ and helps sensor trigger the workloads.","title":"AMQP"},{"location":"setup/amqp/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"contentType\": \"ContentType is the MIME content type\", \"contentEncoding\": \"ContentEncoding is the MIME content encoding\", \"deliveryMode\": \"Delivery mode can be either - non-persistent (1) or persistent (2)\", \"priority\": \"Priority refers to the use - 0 to 9\", \"correlationId\": \"CorrelationId is the correlation identifier\", \"replyTo\": \"ReplyTo is the address to reply to (ex: RPC)\", \"expiration\": \"Expiration refers to message expiration spec\", \"messageId\": \"MessageId is message identifier\", \"timestamp\": \"Timestamp refers to the message timestamp\", \"type\": \"Type refers to the message type name\", \"appId\": \"AppId refers to the application id\", \"exchange\": \"Exchange is basic.publish exchange\", \"routingKey\": \"RoutingKey is basic.publish routing key\", \"body\": \"Body represents the messsage body\", } }","title":"Event Structure"},{"location":"setup/amqp/#setup","text":"Lets set up RabbitMQ locally, apiVersion : v1 kind : Service metadata : labels : component : rabbitmq name : rabbitmq - service spec : ports : - port : 5672 selector : app : taskQueue component : rabbitmq --- apiVersion : v1 kind : ReplicationController metadata : labels : component : rabbitmq name : rabbitmq - controller spec : replicas : 1 template : metadata : labels : app : taskQueue component : rabbitmq spec : containers : - image : rabbitmq name : rabbitmq ports : - containerPort : 5672 resources : limits : cpu : 100 m Make sure the RabbitMQ controller pod is up and running before proceeding further. Expose the RabbitMQ server to local publisher using port-forward , kubectl -n argo-events port-forward <rabbitmq-pod-name> 5672:5672 Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/amqp.yaml Inspect the event-source pod logs to make sure it was able to subscribe to the exchange specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/amqp.yaml Lets set up a rabbitmq publisher. If you don't have pika installed, run, python -m pip install pika --upgrade Open a python REPL and run following code to publish a message on exhange called test . import pika connection = pika . BlockingConnection ( pika . ConnectionParameters ( 'localhost' )) channel = connection . channel () channel . basic_publish ( exchange = 'test' , routing_key = 'hello' , body = '{\"message\": \"hello\"}' ) As soon as you publish a message, sensor will trigger an Argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/amqp/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/aws-sns/","text":"AWS SNS \u00b6 SNS event-source subscribes to AWS SNS topics, listens events and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"header\": \"sns headers\", \"body\": \"body refers to the sns notification data\", } } Setup \u00b6 Create a topic called test using aws cli or AWS SNS console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl -n argo-events apply -f aws-secret.yaml The event-source for AWS SNS creates a pod and exposes it via service. The name for the service is in <event-source-name>-eventsource-svc format. You will need to create an Ingress or Openshift Route for the event-source service so that it can be reached from AWS. You can find more information on Ingress or Route online. Create the event source by running the following command. Make sure to update the URL in the configuration within the event-source. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/aws-sns.yaml Go to SNS settings on AWS and verify the webhook is registered. You can also check it by inspecting the event-source pod logs. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/aws-sns.yaml Publish a message to the SNS topic, and it will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AWS SNS"},{"location":"setup/aws-sns/#aws-sns","text":"SNS event-source subscribes to AWS SNS topics, listens events and helps sensor trigger the workloads.","title":"AWS SNS"},{"location":"setup/aws-sns/#event-structure","text":"The structure of an event dispatched by the event-source over eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"header\": \"sns headers\", \"body\": \"body refers to the sns notification data\", } }","title":"Event Structure"},{"location":"setup/aws-sns/#setup","text":"Create a topic called test using aws cli or AWS SNS console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl -n argo-events apply -f aws-secret.yaml The event-source for AWS SNS creates a pod and exposes it via service. The name for the service is in <event-source-name>-eventsource-svc format. You will need to create an Ingress or Openshift Route for the event-source service so that it can be reached from AWS. You can find more information on Ingress or Route online. Create the event source by running the following command. Make sure to update the URL in the configuration within the event-source. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/aws-sns.yaml Go to SNS settings on AWS and verify the webhook is registered. You can also check it by inspecting the event-source pod logs. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/aws-sns.yaml Publish a message to the SNS topic, and it will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/aws-sns/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/aws-sqs/","text":"AWS SQS \u00b6 SQS event-source listens to messages on AWS SQS queue and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \" context \" : { \" type \" : \" type_of_event_source \" , \" specversion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_event_source \" , \" id \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" datacontenttype \" : \" type_of_data \" , \" subject \" : \" name_of_the_configuration_within_event_source \" }, \" data \" : { \" messageId \" : \" message id \" , // Each message attribute consists of a Name , Type , and Value . For more information , // see Amazon SQS Message Attributes // ( https : // docs . aws . amazon . com / AWSSimpleQueueService / latest / SQSDeveloperGuide / sqs - message - attributes . html ) // in the Amazon Simple Queue Service Developer Guide . \" messageAttributes \" : \" message attributes \" , \" body \" : \" Body is the message data \" , } } Setup \u00b6 Create a queue called test either using aws cli or AWS SQS management console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl -n argo-events apply -f aws-secret.yaml Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/aws-sqs.yaml Inspect the event-source pod logs to make sure it was able to subscribe to the queue specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/aws-sqs.yaml Dispatch a message on sqs queue, aws sqs send - message -- queue - url https : // sqs . us - east - 1 . amazonaws . com / XXXXX / test -- message - body ' {\"message\": \"hello\"} ' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AWS SQS"},{"location":"setup/aws-sqs/#aws-sqs","text":"SQS event-source listens to messages on AWS SQS queue and helps sensor trigger workloads.","title":"AWS SQS"},{"location":"setup/aws-sqs/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \" context \" : { \" type \" : \" type_of_event_source \" , \" specversion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_event_source \" , \" id \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" datacontenttype \" : \" type_of_data \" , \" subject \" : \" name_of_the_configuration_within_event_source \" }, \" data \" : { \" messageId \" : \" message id \" , // Each message attribute consists of a Name , Type , and Value . For more information , // see Amazon SQS Message Attributes // ( https : // docs . aws . amazon . com / AWSSimpleQueueService / latest / SQSDeveloperGuide / sqs - message - attributes . html ) // in the Amazon Simple Queue Service Developer Guide . \" messageAttributes \" : \" message attributes \" , \" body \" : \" Body is the message data \" , } }","title":"Event Structure"},{"location":"setup/aws-sqs/#setup","text":"Create a queue called test either using aws cli or AWS SQS management console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl -n argo-events apply -f aws-secret.yaml Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/aws-sqs.yaml Inspect the event-source pod logs to make sure it was able to subscribe to the queue specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/aws-sqs.yaml Dispatch a message on sqs queue, aws sqs send - message -- queue - url https : // sqs . us - east - 1 . amazonaws . com / XXXXX / test -- message - body ' {\"message\": \"hello\"} ' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/aws-sqs/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/calendar/","text":"Calendar \u00b6 Calendar event-source generates events on either a cron schedule or an interval and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"eventTime\" : { /* UTC time of the event */ }, \"userPayload\" : { /* static payload available in the event source */ }, } } Specification \u00b6 Calendar event-source specification is available here . Setup \u00b6 Install the event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/calendar.yaml The event-source will generate events at every 10 seconds. Let's create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/calendar.yaml Once the sensor pod is in running state, wait for next interval to occur for sensor to trigger workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Calendar"},{"location":"setup/calendar/#calendar","text":"Calendar event-source generates events on either a cron schedule or an interval and helps sensor trigger workloads.","title":"Calendar"},{"location":"setup/calendar/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"eventTime\" : { /* UTC time of the event */ }, \"userPayload\" : { /* static payload available in the event source */ }, } }","title":"Event Structure"},{"location":"setup/calendar/#specification","text":"Calendar event-source specification is available here .","title":"Specification"},{"location":"setup/calendar/#setup","text":"Install the event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/calendar.yaml The event-source will generate events at every 10 seconds. Let's create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/calendar.yaml Once the sensor pod is in running state, wait for next interval to occur for sensor to trigger workflow.","title":"Setup"},{"location":"setup/calendar/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/emitter/","text":"Emitter \u00b6 Emitter event-source subscribes to a channel and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"topic\" : \"name_of_the_topic\" , \"body\" : \"message_payload\" } } Specification \u00b6 Emitter event-source specification is available here . Setup \u00b6 Deploy the emitter in your local K8s cluster, --- apiVersion : v1 kind : Service metadata : name : broker labels : app : broker spec : clusterIP : None ports : - port : 4000 targetPort : 4000 selector : app : broker --- apiVersion : apps / v1 kind : Deployment metadata : name : broker spec : replicas : 1 selector : matchLabels : app : broker template : metadata : labels : app : broker spec : containers : - env : - name : EMITTER_LICENSE value : \" zT83oDV0DWY5_JysbSTPTDr8KB0AAAAAAAAAAAAAAAI \" # This is a test license , DO NOT USE IN PRODUCTION ! - name : EMITTER_CLUSTER_SEED value : \" broker \" - name : EMITTER_CLUSTER_ADVERTISE value : \" private:4000 \" name : broker image : emitter / server : latest ports : - containerPort : 8080 - containerPort : 443 - containerPort : 4000 volumeMounts : - name : broker - volume mountPath : / data volumes : - name : broker - volume hostPath : path : / emitter # directory on host Create the event-source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/emitter.yaml Inspect the event-source pod logs to make sure it was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/emitter.yaml Send a message on emitter channel using one of the clients https://emitter.io/develop/golang/ Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Emitter"},{"location":"setup/emitter/#emitter","text":"Emitter event-source subscribes to a channel and helps sensor trigger the workloads.","title":"Emitter"},{"location":"setup/emitter/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"topic\" : \"name_of_the_topic\" , \"body\" : \"message_payload\" } }","title":"Event Structure"},{"location":"setup/emitter/#specification","text":"Emitter event-source specification is available here .","title":"Specification"},{"location":"setup/emitter/#setup","text":"Deploy the emitter in your local K8s cluster, --- apiVersion : v1 kind : Service metadata : name : broker labels : app : broker spec : clusterIP : None ports : - port : 4000 targetPort : 4000 selector : app : broker --- apiVersion : apps / v1 kind : Deployment metadata : name : broker spec : replicas : 1 selector : matchLabels : app : broker template : metadata : labels : app : broker spec : containers : - env : - name : EMITTER_LICENSE value : \" zT83oDV0DWY5_JysbSTPTDr8KB0AAAAAAAAAAAAAAAI \" # This is a test license , DO NOT USE IN PRODUCTION ! - name : EMITTER_CLUSTER_SEED value : \" broker \" - name : EMITTER_CLUSTER_ADVERTISE value : \" private:4000 \" name : broker image : emitter / server : latest ports : - containerPort : 8080 - containerPort : 443 - containerPort : 4000 volumeMounts : - name : broker - volume mountPath : / data volumes : - name : broker - volume hostPath : path : / emitter # directory on host Create the event-source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/emitter.yaml Inspect the event-source pod logs to make sure it was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/emitter.yaml Send a message on emitter channel using one of the clients https://emitter.io/develop/golang/ Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/emitter/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/file/","text":"File \u00b6 File event-source listens to file system events and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"name\": \"Relative path to the file or directory\", \"op\": \"File operation that triggered the event\" // Create, Write, Remove, Rename, Chmod } } Specification \u00b6 File event-source specification is available here . Setup \u00b6 Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/file.yaml The event source has configuration to listen to file system events for test-data directory and file called x.txt . Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/file.yaml Log into the event-source pod by running following command, kubectl - n argo - events exec - it < event - source - pod - name > - c file - events -- / bin / bash Let's create a file called x.txt under test-data directory in the event-source pod. cd test-data cat <<EOF > x.txt hello EOF Once you create file x.txt , the sensor will trigger argo workflow. Run argo list to find the workflow. For real-world use cases, you should use PersistentVolumeClaim. Troubleshoot \u00b6 Please read the FAQ .","title":"File"},{"location":"setup/file/#file","text":"File event-source listens to file system events and helps sensor trigger workloads.","title":"File"},{"location":"setup/file/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"name\": \"Relative path to the file or directory\", \"op\": \"File operation that triggered the event\" // Create, Write, Remove, Rename, Chmod } }","title":"Event Structure"},{"location":"setup/file/#specification","text":"File event-source specification is available here .","title":"Specification"},{"location":"setup/file/#setup","text":"Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/file.yaml The event source has configuration to listen to file system events for test-data directory and file called x.txt . Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/file.yaml Log into the event-source pod by running following command, kubectl - n argo - events exec - it < event - source - pod - name > - c file - events -- / bin / bash Let's create a file called x.txt under test-data directory in the event-source pod. cd test-data cat <<EOF > x.txt hello EOF Once you create file x.txt , the sensor will trigger argo workflow. Run argo list to find the workflow. For real-world use cases, you should use PersistentVolumeClaim.","title":"Setup"},{"location":"setup/file/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/gcp-pub-sub/","text":"GCP Pub/Sub \u00b6 GCP Pub/Sub event-source subscribes to messages published by GCP publisher and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"id\" : \"message id\" , // Attributes represents the key-value pairs the current message // is labelled with. \"attributes\" : \"key-values\" , \"publishTime\" : \"// The time at which the message was published\" , \"body\" : \"body refers to the message data\" , } } Specification \u00b6 GCP Pub/Sub event-source specification is available here . Setup \u00b6 Fetch the project credentials JSON file from GCP console. If you use Workload Identity, you can skip this and next steps. Create a K8s secret called gcp-credentials to store the credentials file yaml apiVersion: v1 data: key.json: <YOUR_CREDENTIALS_STRING_FROM_JSON_FILE> kind: Secret metadata: name: gcp-credentials namespace: argo-events type: Opaque Create the event source by running the following command. sh kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/gcp-pubsub.yaml If you use Workload Identity, omit credentialSecret field. Instead don't forget to configure appropriate service account (see example ). Inspect the event-source pod logs to make sure it was able to subscribe to the topic. Create the sensor by running the following command, sh kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/gcp-pubsub.yaml Publish a message from GCP Pub/Sub console. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Subscription, topic and service account preparetion \u00b6 You can use exisiting subscriptions/topics, or let Argo Events create them. Here's the table of which fields are required in the configuration file and what permissions are needed for service account. Actions Required configuration fields Necessary permissions for service account Example role Use existing subscription Existing SubscriptionID pubsub.subscriptions.consume for the subscription roles/pubsub.subscriber Use existing subscription and verify topic Existing SubscriptionID and its Topic Above + pubsub.subscriptions.get for the subscription roles/pubsub.subscriber + roles/pubsub.viewer Create subscription for existing topic Existing Topic ( SubscriptionID is optional\u2020) Above + pubsub.subscriptions.create for the project pubsub.topics.attachSubscription for the topic roles/pubsub.subscriber + roles/pubsub.editor Create topic and subscription Non-existing Topic ( SubscriptionID is optional\u2020) Above + pubsub.topic.create for the project roles/pubsub.subscriber + roles/pubsub.editor \u2020 If you omit SubscriptionID , a generated hash value is used. For more details about access control, refer to GCP documents: Access control | Cloud Pub/Sub Documentation | Google Cloud \u29c9 Troubleshoot \u00b6 Please read the FAQ .","title":"GCP Pub/Sub"},{"location":"setup/gcp-pub-sub/#gcp-pubsub","text":"GCP Pub/Sub event-source subscribes to messages published by GCP publisher and helps sensor trigger workloads.","title":"GCP Pub/Sub"},{"location":"setup/gcp-pub-sub/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"id\" : \"message id\" , // Attributes represents the key-value pairs the current message // is labelled with. \"attributes\" : \"key-values\" , \"publishTime\" : \"// The time at which the message was published\" , \"body\" : \"body refers to the message data\" , } }","title":"Event Structure"},{"location":"setup/gcp-pub-sub/#specification","text":"GCP Pub/Sub event-source specification is available here .","title":"Specification"},{"location":"setup/gcp-pub-sub/#setup","text":"Fetch the project credentials JSON file from GCP console. If you use Workload Identity, you can skip this and next steps. Create a K8s secret called gcp-credentials to store the credentials file yaml apiVersion: v1 data: key.json: <YOUR_CREDENTIALS_STRING_FROM_JSON_FILE> kind: Secret metadata: name: gcp-credentials namespace: argo-events type: Opaque Create the event source by running the following command. sh kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/gcp-pubsub.yaml If you use Workload Identity, omit credentialSecret field. Instead don't forget to configure appropriate service account (see example ). Inspect the event-source pod logs to make sure it was able to subscribe to the topic. Create the sensor by running the following command, sh kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/gcp-pubsub.yaml Publish a message from GCP Pub/Sub console. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/gcp-pub-sub/#subscription-topic-and-service-account-preparetion","text":"You can use exisiting subscriptions/topics, or let Argo Events create them. Here's the table of which fields are required in the configuration file and what permissions are needed for service account. Actions Required configuration fields Necessary permissions for service account Example role Use existing subscription Existing SubscriptionID pubsub.subscriptions.consume for the subscription roles/pubsub.subscriber Use existing subscription and verify topic Existing SubscriptionID and its Topic Above + pubsub.subscriptions.get for the subscription roles/pubsub.subscriber + roles/pubsub.viewer Create subscription for existing topic Existing Topic ( SubscriptionID is optional\u2020) Above + pubsub.subscriptions.create for the project pubsub.topics.attachSubscription for the topic roles/pubsub.subscriber + roles/pubsub.editor Create topic and subscription Non-existing Topic ( SubscriptionID is optional\u2020) Above + pubsub.topic.create for the project roles/pubsub.subscriber + roles/pubsub.editor \u2020 If you omit SubscriptionID , a generated hash value is used. For more details about access control, refer to GCP documents: Access control | Cloud Pub/Sub Documentation | Google Cloud \u29c9","title":"Subscription, topic and service account preparetion"},{"location":"setup/gcp-pub-sub/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/github/","text":"GitHub \u00b6 GitHub event-source programmatically configures webhooks for projects on GitHub and helps sensor trigger the workloads on events. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"Body is the github event data\", \"headers\": \"Headers from the Gitlab event\", } } Specification \u00b6 GitHub event-source specification is available here . Example event-source yaml file is here . Setup \u00b6 Create an API token if you don't have one. Follow instructions to create a new GitHub API Token. Grant it the repo_hook permissions. Base64 encode your api token key, echo -n <api-token-key> | base64 Create a secret called github-access . apiVersion : v1 kind : Secret metadata : name : github - access type : Opaque data : token : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl -n argo-events apply -f github-access.yaml The event-source for GitHub creates a pod and exposes it via service. The name for the service is in <event-source-name>-eventsource-svc format. You will need to create an Ingress or Openshift Route for the event-source service so that it can be reached from GitHub. You can find more information on Ingress or Route online. Create the event source by running the following command. Make sure to replace the url field. kubectl apply -n argo-events -f <event-source-file-updated-in-previous-step> Go to Webhooks under your project settings on GitHub and verify the webhook is registered. You can also do the same by looking at the event-source pod logs. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/github.yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GitHub"},{"location":"setup/github/#github","text":"GitHub event-source programmatically configures webhooks for projects on GitHub and helps sensor trigger the workloads on events.","title":"GitHub"},{"location":"setup/github/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"Body is the github event data\", \"headers\": \"Headers from the Gitlab event\", } }","title":"Event Structure"},{"location":"setup/github/#specification","text":"GitHub event-source specification is available here . Example event-source yaml file is here .","title":"Specification"},{"location":"setup/github/#setup","text":"Create an API token if you don't have one. Follow instructions to create a new GitHub API Token. Grant it the repo_hook permissions. Base64 encode your api token key, echo -n <api-token-key> | base64 Create a secret called github-access . apiVersion : v1 kind : Secret metadata : name : github - access type : Opaque data : token : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl -n argo-events apply -f github-access.yaml The event-source for GitHub creates a pod and exposes it via service. The name for the service is in <event-source-name>-eventsource-svc format. You will need to create an Ingress or Openshift Route for the event-source service so that it can be reached from GitHub. You can find more information on Ingress or Route online. Create the event source by running the following command. Make sure to replace the url field. kubectl apply -n argo-events -f <event-source-file-updated-in-previous-step> Go to Webhooks under your project settings on GitHub and verify the webhook is registered. You can also do the same by looking at the event-source pod logs. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/github.yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/github/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/gitlab/","text":"GitLab \u00b6 GitLab event-source programmatically configures webhooks for projects on GitLab and helps sensor trigger the workloads upon events. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"Body is the gitlab event data\", \"headers\": \"Headers from the Gitlab event\", } } Specification \u00b6 GitLab event-source specification is available here . Example event-source yaml file is here . Setup \u00b6 Create an API token if you don't have one. Follow instructions to create a new GitLab API Token. Grant it the api permissions. Base64 encode your api token key, echo -n <api-token-key> | base64 Create a secret called gitlab-access . apiVersion : v1 kind : Secret metadata : name : gitlab - access type : Opaque data : token : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster. kubectl -n argo-events apply -f gitlab-access.yaml The event-source for GitLab creates a pod and exposes it via service. The name for the service is in <event-source-name>-eventsource-svc format. You will need to create an Ingress or Openshift Route for the event-source service so that it can be reached from GitLab. You can find more information on Ingress or Route online. Create the event source by running the following command. Make sure to update url field. kubectl apply -n argo-events -f <event-source-file-updated-in-previous-step> Go to Webhooks under your project settings on GitLab and verify the webhook is registered. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/gitlab.yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GitLab"},{"location":"setup/gitlab/#gitlab","text":"GitLab event-source programmatically configures webhooks for projects on GitLab and helps sensor trigger the workloads upon events.","title":"GitLab"},{"location":"setup/gitlab/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"Body is the gitlab event data\", \"headers\": \"Headers from the Gitlab event\", } }","title":"Event Structure"},{"location":"setup/gitlab/#specification","text":"GitLab event-source specification is available here . Example event-source yaml file is here .","title":"Specification"},{"location":"setup/gitlab/#setup","text":"Create an API token if you don't have one. Follow instructions to create a new GitLab API Token. Grant it the api permissions. Base64 encode your api token key, echo -n <api-token-key> | base64 Create a secret called gitlab-access . apiVersion : v1 kind : Secret metadata : name : gitlab - access type : Opaque data : token : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster. kubectl -n argo-events apply -f gitlab-access.yaml The event-source for GitLab creates a pod and exposes it via service. The name for the service is in <event-source-name>-eventsource-svc format. You will need to create an Ingress or Openshift Route for the event-source service so that it can be reached from GitLab. You can find more information on Ingress or Route online. Create the event source by running the following command. Make sure to update url field. kubectl apply -n argo-events -f <event-source-file-updated-in-previous-step> Go to Webhooks under your project settings on GitLab and verify the webhook is registered. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/gitlab.yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/gitlab/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/kafka/","text":"Kafka \u00b6 Kafka event-source listens to messages on topics and helps the sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"topic\": \"kafka_topic\", \"partition\": \"partition_number\", \"body\": \"message_body\", \"timestamp\": \"timestamp_of_the_message\" } } Specification \u00b6 Kafka event-source specification is available here . Setup \u00b6 Make sure to set up the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/kafka.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/kafka.yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Kafka"},{"location":"setup/kafka/#kafka","text":"Kafka event-source listens to messages on topics and helps the sensor trigger workloads.","title":"Kafka"},{"location":"setup/kafka/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"topic\": \"kafka_topic\", \"partition\": \"partition_number\", \"body\": \"message_body\", \"timestamp\": \"timestamp_of_the_message\" } }","title":"Event Structure"},{"location":"setup/kafka/#specification","text":"Kafka event-source specification is available here .","title":"Specification"},{"location":"setup/kafka/#setup","text":"Make sure to set up the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/kafka.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/kafka.yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/kafka/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/minio/","text":"Minio \u00b6 Minio event-source listens to minio bucket notifications and helps sensor trigger the workloads. Note : Minio event-source is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, please set up the AWS SNS event-source. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { notification: [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } } Setup \u00b6 Make sure to have the minio server deployed and reachable from the event-source. If you are running Minio locally, make sure to port-forward to minio pod in order to make the service available outside local K8s cluster. kubectl -n argo-events port-forward <minio-pod-name> 9000:9000 Configure the minio client mc . mc config host add minio http://localhost:9000 minio minio123 Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events The event source we are going to use configures notifications for a bucket called input . mc mb minio/input Let's install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/minio.yaml Let's create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/minio.yaml Create a file named and hello-world.txt and upload it onto to the input bucket. This will trigger the argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Minio"},{"location":"setup/minio/#minio","text":"Minio event-source listens to minio bucket notifications and helps sensor trigger the workloads. Note : Minio event-source is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, please set up the AWS SNS event-source.","title":"Minio"},{"location":"setup/minio/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { notification: [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } }","title":"Event Structure"},{"location":"setup/minio/#setup","text":"Make sure to have the minio server deployed and reachable from the event-source. If you are running Minio locally, make sure to port-forward to minio pod in order to make the service available outside local K8s cluster. kubectl -n argo-events port-forward <minio-pod-name> 9000:9000 Configure the minio client mc . mc config host add minio http://localhost:9000 minio minio123 Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events The event source we are going to use configures notifications for a bucket called input . mc mb minio/input Let's install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/minio.yaml Let's create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/minio.yaml Create a file named and hello-world.txt and upload it onto to the input bucket. This will trigger the argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/minio/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/mqtt/","text":"MQTT \u00b6 The event-source listens to messages over MQTT and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"topic\" : \"Topic refers to the MQTT topic name\" , \"messageId\" : \"MessageId is the unique ID for the message\" , \"body\" : \"Body is the message payload\" } } Specification \u00b6 MQTT event-source specification is available here . Setup \u00b6 Make sure to set up the MQTT Broker and Bridge in Kubernetes if you don't already have one. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/mqtt.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/mqtt.yaml Send message by using MQTT client. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"MQTT"},{"location":"setup/mqtt/#mqtt","text":"The event-source listens to messages over MQTT and helps sensor trigger the workloads.","title":"MQTT"},{"location":"setup/mqtt/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"topic\" : \"Topic refers to the MQTT topic name\" , \"messageId\" : \"MessageId is the unique ID for the message\" , \"body\" : \"Body is the message payload\" } }","title":"Event Structure"},{"location":"setup/mqtt/#specification","text":"MQTT event-source specification is available here .","title":"Specification"},{"location":"setup/mqtt/#setup","text":"Make sure to set up the MQTT Broker and Bridge in Kubernetes if you don't already have one. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/mqtt.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/mqtt.yaml Send message by using MQTT client. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/mqtt/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nats/","text":"NATS \u00b6 NATS event-source listens to NATS subject notifications and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"subject\" : \"name_of_the_nats_subject\" , \"body\" : \"message_payload\" } } Specification \u00b6 NATS event-source specification is available here . Setup \u00b6 Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion: v1 kind: Service metadata: name: nats namespace: argo-events labels: component: nats spec: selector: component: nats type: ClusterIP ports: - name: client port: 4222 - name: cluster port: 6222 - name: monitor port: 8222 --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: nats namespace: argo-events labels: component: nats spec: serviceName: nats replicas: 1 template: metadata: labels: component: nats spec: serviceAccountName: argo-events-sa containers: - name: nats image: nats:latest ports: - containerPort: 4222 name: client - containerPort: 6222 name: cluster - containerPort: 8222 name: monitor livenessProbe: httpGet: path: / port: 8222 initialDelaySeconds: 10 timeoutSeconds: 5 Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/nats.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/nats.yaml If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl -n argo-events port-forward <nats-pod-name> 4222:4222 Publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost foo '{\"message\": \"hello\"}' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"NATS"},{"location":"setup/nats/#nats","text":"NATS event-source listens to NATS subject notifications and helps sensor trigger the workloads.","title":"NATS"},{"location":"setup/nats/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"subject\" : \"name_of_the_nats_subject\" , \"body\" : \"message_payload\" } }","title":"Event Structure"},{"location":"setup/nats/#specification","text":"NATS event-source specification is available here .","title":"Specification"},{"location":"setup/nats/#setup","text":"Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion: v1 kind: Service metadata: name: nats namespace: argo-events labels: component: nats spec: selector: component: nats type: ClusterIP ports: - name: client port: 4222 - name: cluster port: 6222 - name: monitor port: 8222 --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: nats namespace: argo-events labels: component: nats spec: serviceName: nats replicas: 1 template: metadata: labels: component: nats spec: serviceAccountName: argo-events-sa containers: - name: nats image: nats:latest ports: - containerPort: 4222 name: client - containerPort: 6222 name: cluster - containerPort: 8222 name: monitor livenessProbe: httpGet: path: / port: 8222 initialDelaySeconds: 10 timeoutSeconds: 5 Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/nats.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/nats.yaml If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl -n argo-events port-forward <nats-pod-name> 4222:4222 Publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost foo '{\"message\": \"hello\"}' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nats/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nsq/","text":"NSQ \u00b6 NSQ event-source subscribes to nsq pub/sub notifications and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"Body is the message data\", \"timestamp\": \"timestamp of the message\", \"nsqdAddress\": \"NSQDAddress is the address of the nsq host\" } } Specification \u00b6 NSQ event-source is available here . Setup \u00b6 Deploy NSQ on local K8s cluster apiVersion : v1 kind : Service metadata : name : nsqlookupd labels : app : nsq spec : ports : - port : 4160 targetPort : 4160 name : tcp - port : 4161 targetPort : 4161 name : http clusterIP : None selector : app : nsq component : nsqlookupd --- apiVersion : v1 kind : Service metadata : name : nsqd labels : app : nsq spec : ports : - port : 4150 targetPort : 4150 name : tcp - port : 4151 targetPort : 4151 name : http clusterIP : None selector : app : nsq component : nsqd --- apiVersion : v1 kind : Service metadata : name : nsqadmin labels : app : nsq spec : ports : - port : 4170 targetPort : 4170 name : tcp - port : 4171 targetPort : 4171 name : http selector : app : nsq component : nsqadmin --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nsqlookupd spec : serviceName : \"nsqlookupd\" replicas : 1 updateStrategy : type : RollingUpdate template : metadata : labels : app : nsq component : nsqlookupd spec : containers : - name : nsqlookupd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4160 name : tcp - containerPort : 4161 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 command : - / nsqlookupd terminationGracePeriodSeconds : 5 --- apiVersion : apps / v1beta1 kind : Deployment metadata : name : nsqd spec : replicas : 1 selector : matchLabels : app : nsq component : nsqd template : metadata : labels : app : nsq component : nsqd spec : containers : - name : nsqd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4150 name : tcp - containerPort : 4151 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 volumeMounts : - name : datadir mountPath : / data command : - / nsqd - - data - path - / data - - lookupd - tcp - address - nsqlookupd . argo - events . svc : 4160 - - broadcast - address - nsqd . argo - events . svc env : - name : HOSTNAME valueFrom : fieldRef : fieldPath : metadata . name terminationGracePeriodSeconds : 5 volumes : - name : datadir emptyDir : {} --- apiVersion : extensions / v1beta1 kind : Deployment metadata : name : nsqadmin spec : replicas : 1 template : metadata : labels : app : nsq component : nsqadmin spec : containers : - name : nsqadmin image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4170 name : tcp - containerPort : 4171 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 10 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 command : - / nsqadmin - - lookupd - http - address - nsqlookupd . argo - events . svc : 4161 terminationGracePeriodSeconds : 5 Expose NSQD by kubectl port-forward , kubectl -n argo-events port-forward service/nsqd 4151:4151 Create topic hello and channel my-channel curl -X POST 'http://localhost:4151/topic/create?topic=hello' curl -X POST 'http://localhost:4151/channel/create?topic=hello&channel=my-channel' Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/nsq.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/nsq.yaml Publish a message on topic hello and channel my-channel , curl -d '{\"message\": \"hello\"}' 'http://localhost:4151/pub?topic=hello&channel=my-channel' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"NSQ"},{"location":"setup/nsq/#nsq","text":"NSQ event-source subscribes to nsq pub/sub notifications and helps sensor trigger the workloads.","title":"NSQ"},{"location":"setup/nsq/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"Body is the message data\", \"timestamp\": \"timestamp of the message\", \"nsqdAddress\": \"NSQDAddress is the address of the nsq host\" } }","title":"Event Structure"},{"location":"setup/nsq/#specification","text":"NSQ event-source is available here .","title":"Specification"},{"location":"setup/nsq/#setup","text":"Deploy NSQ on local K8s cluster apiVersion : v1 kind : Service metadata : name : nsqlookupd labels : app : nsq spec : ports : - port : 4160 targetPort : 4160 name : tcp - port : 4161 targetPort : 4161 name : http clusterIP : None selector : app : nsq component : nsqlookupd --- apiVersion : v1 kind : Service metadata : name : nsqd labels : app : nsq spec : ports : - port : 4150 targetPort : 4150 name : tcp - port : 4151 targetPort : 4151 name : http clusterIP : None selector : app : nsq component : nsqd --- apiVersion : v1 kind : Service metadata : name : nsqadmin labels : app : nsq spec : ports : - port : 4170 targetPort : 4170 name : tcp - port : 4171 targetPort : 4171 name : http selector : app : nsq component : nsqadmin --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nsqlookupd spec : serviceName : \"nsqlookupd\" replicas : 1 updateStrategy : type : RollingUpdate template : metadata : labels : app : nsq component : nsqlookupd spec : containers : - name : nsqlookupd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4160 name : tcp - containerPort : 4161 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 command : - / nsqlookupd terminationGracePeriodSeconds : 5 --- apiVersion : apps / v1beta1 kind : Deployment metadata : name : nsqd spec : replicas : 1 selector : matchLabels : app : nsq component : nsqd template : metadata : labels : app : nsq component : nsqd spec : containers : - name : nsqd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4150 name : tcp - containerPort : 4151 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 volumeMounts : - name : datadir mountPath : / data command : - / nsqd - - data - path - / data - - lookupd - tcp - address - nsqlookupd . argo - events . svc : 4160 - - broadcast - address - nsqd . argo - events . svc env : - name : HOSTNAME valueFrom : fieldRef : fieldPath : metadata . name terminationGracePeriodSeconds : 5 volumes : - name : datadir emptyDir : {} --- apiVersion : extensions / v1beta1 kind : Deployment metadata : name : nsqadmin spec : replicas : 1 template : metadata : labels : app : nsq component : nsqadmin spec : containers : - name : nsqadmin image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4170 name : tcp - containerPort : 4171 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 10 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 command : - / nsqadmin - - lookupd - http - address - nsqlookupd . argo - events . svc : 4161 terminationGracePeriodSeconds : 5 Expose NSQD by kubectl port-forward , kubectl -n argo-events port-forward service/nsqd 4151:4151 Create topic hello and channel my-channel curl -X POST 'http://localhost:4151/topic/create?topic=hello' curl -X POST 'http://localhost:4151/channel/create?topic=hello&channel=my-channel' Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/nsq.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/nsq.yaml Publish a message on topic hello and channel my-channel , curl -d '{\"message\": \"hello\"}' 'http://localhost:4151/pub?topic=hello&channel=my-channel' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nsq/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/pulsar/","text":"Pulsar \u00b6 Pulsar event-source subscribes to the topics, listens events and helps sensor trigger the workflows. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"body is the message data\", \"publishTime\": \"timestamp of the message\", \"key\": \"message key\" } } Specification \u00b6 Pulsar event-source is available here . Setup \u00b6 To test locally, deploy a standalone Pulsar, apiVersion : apps / v1 kind : Deployment metadata : name : pulsar labels : app : pulsar spec : replicas : 1 template : metadata : name : pulsar labels : app : pulsar spec : containers : - name : pulsar image : apachepulsar / pulsar : 2.4 . 1 command : - bin / pulsar - standalone imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /pulsar/ data name : datadir restartPolicy : Always volumes : - name : datadir emptyDir : {} selector : matchLabels : app : pulsar --- apiVersion : v1 kind : Service metadata : name : pulsar spec : selector : app : pulsar ports : - port : 8080 targetPort : 8080 name : http - port : 6650 name : another targetPort : 6650 type : LoadBalancer Port forward to the pulsar pod using kubectl for port 6650. For production deployment, follow the official Pulsar documentation online. Deploy the eventsource, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/pulsar.yaml Deploy the sensor, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/pulsar.yaml Publish a message on topic test . Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Pulsar"},{"location":"setup/pulsar/#pulsar","text":"Pulsar event-source subscribes to the topics, listens events and helps sensor trigger the workflows.","title":"Pulsar"},{"location":"setup/pulsar/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": \"body is the message data\", \"publishTime\": \"timestamp of the message\", \"key\": \"message key\" } }","title":"Event Structure"},{"location":"setup/pulsar/#specification","text":"Pulsar event-source is available here .","title":"Specification"},{"location":"setup/pulsar/#setup","text":"To test locally, deploy a standalone Pulsar, apiVersion : apps / v1 kind : Deployment metadata : name : pulsar labels : app : pulsar spec : replicas : 1 template : metadata : name : pulsar labels : app : pulsar spec : containers : - name : pulsar image : apachepulsar / pulsar : 2.4 . 1 command : - bin / pulsar - standalone imagePullPolicy : IfNotPresent volumeMounts : - mountPath : /pulsar/ data name : datadir restartPolicy : Always volumes : - name : datadir emptyDir : {} selector : matchLabels : app : pulsar --- apiVersion : v1 kind : Service metadata : name : pulsar spec : selector : app : pulsar ports : - port : 8080 targetPort : 8080 name : http - port : 6650 name : another targetPort : 6650 type : LoadBalancer Port forward to the pulsar pod using kubectl for port 6650. For production deployment, follow the official Pulsar documentation online. Deploy the eventsource, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/pulsar.yaml Deploy the sensor, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/pulsar.yaml Publish a message on topic test . Run argo list to find the workflow.","title":"Setup"},{"location":"setup/pulsar/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/redis/","text":"Redis \u00b6 Redis event-source subscribes to Redis publisher and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"channel\": \"Subscription channel\", \"pattern\": \"Message pattern\", \"body\": \"message body\" // string } } Specification \u00b6 Redis event-source specification is available here . Setup \u00b6 Follow the documentation to set up Redis database. Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/redis.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/redis.yaml Log into redis pod using kubectl , kubectl - n argo - events exec - it < redis - pod - name > - c < redis - container - name > -- / bin / bash Run redis-cli and publish a message on FOO channel. PUBLISH FOO hello Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Redis"},{"location":"setup/redis/#redis","text":"Redis event-source subscribes to Redis publisher and helps sensor trigger workloads.","title":"Redis"},{"location":"setup/redis/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"channel\": \"Subscription channel\", \"pattern\": \"Message pattern\", \"body\": \"message body\" // string } }","title":"Event Structure"},{"location":"setup/redis/#specification","text":"Redis event-source specification is available here .","title":"Specification"},{"location":"setup/redis/#setup","text":"Follow the documentation to set up Redis database. Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/redis.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/redis.yaml Log into redis pod using kubectl , kubectl - n argo - events exec - it < redis - pod - name > - c < redis - container - name > -- / bin / bash Run redis-cli and publish a message on FOO channel. PUBLISH FOO hello Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/redis/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/resource/","text":"Resource \u00b6 Resource event-source watches change notifications for K8s object and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"type\": \"type_of_the_event\", // ADD, UPDATE or DELETE \"body\": \"resource_body\", // JSON format \"group\": \"resource_group_name\", \"version\": \"resource_version_name\", \"resource\": \"resource_name\" } } Specification \u00b6 Resource event-source specification is available here . Setup \u00b6 Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/resource.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/resource.yaml The event source we created in step 1 contains configuration which makes the event-source listen to Argo workflows marked with label app: my-workflow . Lets create a workflow called my-workflow with label app: my-workflow apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : name : my - workflow labels : app : my - workflow spec : entrypoint : whalesay templates : - name : whalesay container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"hello world\" ] Once the my-workflow is created, the sensor will trigger the workflow. Run argo list to list the triggered workflow. List Options \u00b6 The Resource Event-Source allows to configure the list options through labels and field selectors for setting up a watch on objects. In the example above, we had set up the list option as follows, filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # labels provide listing options to K8s API to watch objects labels : - key : app # Supported operations like == , != , etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # label - selectors for more info . # optional . operation : \" == \" value : my - workflow The key-operation-value items under the filter -> labels are used by the event-source to filter the objects that are eligible for the watch. So, in the present case, the event-source will set up a watch for those objects who have label \"app: my-workflow\". You can add more key-operation-value items to the list as per your use-case. Similarly, you can pass field selectors to the watch list options, e.g., filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # fields provide listing options to K8s API to watch objects fields : - key : metadata . name # Supported operations like == , != , <= , >= etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / field - selectors / for more info . # optional . operation : == value : my - workflow Note: The label and fields under filter are used at the time of setting up the watch by the event-source. If you want to filter the objects based on the annotations or some other fields, use the Data Filters available in the sensor. Troubleshoot \u00b6 Please read the FAQ .","title":"Resource"},{"location":"setup/resource/#resource","text":"Resource event-source watches change notifications for K8s object and helps sensor trigger the workloads.","title":"Resource"},{"location":"setup/resource/#event-structure","text":"The structure of an event dispatched by the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"type\": \"type_of_the_event\", // ADD, UPDATE or DELETE \"body\": \"resource_body\", // JSON format \"group\": \"resource_group_name\", \"version\": \"resource_version_name\", \"resource\": \"resource_name\" } }","title":"Event Structure"},{"location":"setup/resource/#specification","text":"Resource event-source specification is available here .","title":"Specification"},{"location":"setup/resource/#setup","text":"Create the event source by running the following command. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/resource.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/resource.yaml The event source we created in step 1 contains configuration which makes the event-source listen to Argo workflows marked with label app: my-workflow . Lets create a workflow called my-workflow with label app: my-workflow apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : name : my - workflow labels : app : my - workflow spec : entrypoint : whalesay templates : - name : whalesay container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"hello world\" ] Once the my-workflow is created, the sensor will trigger the workflow. Run argo list to list the triggered workflow.","title":"Setup"},{"location":"setup/resource/#list-options","text":"The Resource Event-Source allows to configure the list options through labels and field selectors for setting up a watch on objects. In the example above, we had set up the list option as follows, filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # labels provide listing options to K8s API to watch objects labels : - key : app # Supported operations like == , != , etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # label - selectors for more info . # optional . operation : \" == \" value : my - workflow The key-operation-value items under the filter -> labels are used by the event-source to filter the objects that are eligible for the watch. So, in the present case, the event-source will set up a watch for those objects who have label \"app: my-workflow\". You can add more key-operation-value items to the list as per your use-case. Similarly, you can pass field selectors to the watch list options, e.g., filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # fields provide listing options to K8s API to watch objects fields : - key : metadata . name # Supported operations like == , != , <= , >= etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / field - selectors / for more info . # optional . operation : == value : my - workflow Note: The label and fields under filter are used at the time of setting up the watch by the event-source. If you want to filter the objects based on the annotations or some other fields, use the Data Filters available in the sensor.","title":"List Options"},{"location":"setup/resource/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/webhook/","text":"Webhook \u00b6 Webhook event-source exposes a http server and allows external entities to trigger workloads via http requests. Event Structure \u00b6 The structure of an event dispatched by the event-source to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"header\" : { /* the headers from the request received by the event - source from the external entity */ }, \"body\" : { /* the payload of the request received by the event - source from the external entity */ }, } } Specification \u00b6 Webhook event-source specification is available here . Setup \u00b6 Install the event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml The event-source pod is listening for HTTP requests on port 12000 and endpoint /example . It's time to create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml Once the sensor pod is in running state, test the setup by sending a POST request to event-source service. Troubleshoot \u00b6 Please read the FAQ .","title":"Webhook"},{"location":"setup/webhook/#webhook","text":"Webhook event-source exposes a http server and allows external entities to trigger workloads via http requests.","title":"Webhook"},{"location":"setup/webhook/#event-structure","text":"The structure of an event dispatched by the event-source to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_event_source\" , \"specversion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_event_source\" , \"id\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"datacontenttype\" : \"type_of_data\" , \"subject\" : \"name_of_the_configuration_within_event_source\" }, \"data\" : { \"header\" : { /* the headers from the request received by the event - source from the external entity */ }, \"body\" : { /* the payload of the request received by the event - source from the external entity */ }, } }","title":"Event Structure"},{"location":"setup/webhook/#specification","text":"Webhook event-source specification is available here .","title":"Specification"},{"location":"setup/webhook/#setup","text":"Install the event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml The event-source pod is listening for HTTP requests on port 12000 and endpoint /example . It's time to create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml Once the sensor pod is in running state, test the setup by sending a POST request to event-source service.","title":"Setup"},{"location":"setup/webhook/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"triggers/argo-workflow/","text":"Argo Workflow Trigger \u00b6 Argo workflow is K8s custom resource which help orchestrating parallel jobs on Kubernetes. Trigger a workflow \u00b6 Make sure to have the eventbus deployed in the namespace. We will use webhook event-source and sensor to trigger an Argo workflow. Let's set up a webhook event-source to process incoming requests. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml Let's expose the webhook event-source pod using port-forward so that we can make a request to it. kubectl -n argo-events port-forward <name-of-event-source-pod> 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example List the workflow using argo list . Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the Argo workflow trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the workflow object values on the fly. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the triggered Argo workflow object and decide whether to stop or continue sensor. Take a look at K8s Trigger Policy . Argo CLI \u00b6 Although the sensor defined above lets you trigger an Argo workflow, it doesn't have the ability to leverage the functionality provided by the Argo CLI such as, Submit Resubmit Resume Retry Suspend To make use of Argo CLI operations, The sensor provides the argoWorkflow trigger template, argoWorkflow: group: argoproj.io version: v1alpha1 resource: workflows operation: submit # submit, resubmit, resume, retry or suspend Complete example is available here .","title":"Argo Workflow Trigger"},{"location":"triggers/argo-workflow/#argo-workflow-trigger","text":"Argo workflow is K8s custom resource which help orchestrating parallel jobs on Kubernetes.","title":"Argo Workflow Trigger"},{"location":"triggers/argo-workflow/#trigger-a-workflow","text":"Make sure to have the eventbus deployed in the namespace. We will use webhook event-source and sensor to trigger an Argo workflow. Let's set up a webhook event-source to process incoming requests. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml Let's expose the webhook event-source pod using port-forward so that we can make a request to it. kubectl -n argo-events port-forward <name-of-event-source-pod> 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example List the workflow using argo list .","title":"Trigger a workflow"},{"location":"triggers/argo-workflow/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the Argo workflow trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the workflow object values on the fly. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/argo-workflow/#policy","text":"Trigger policy helps you determine the status of the triggered Argo workflow object and decide whether to stop or continue sensor. Take a look at K8s Trigger Policy .","title":"Policy"},{"location":"triggers/argo-workflow/#argo-cli","text":"Although the sensor defined above lets you trigger an Argo workflow, it doesn't have the ability to leverage the functionality provided by the Argo CLI such as, Submit Resubmit Resume Retry Suspend To make use of Argo CLI operations, The sensor provides the argoWorkflow trigger template, argoWorkflow: group: argoproj.io version: v1alpha1 resource: workflows operation: submit # submit, resubmit, resume, retry or suspend Complete example is available here .","title":"Argo CLI"},{"location":"triggers/aws-lambda/","text":"AWS Lambda \u00b6 AWS Lambda provides a tremendous value, but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS. Trigger A Simple Lambda \u00b6 Make sure to have eventbus deployed in the namespace. Make sure your AWS account has permissions to execute Lambda. More info on AWS permissions is available here . Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Create a basic lambda function called hello either using AWS cli or console. exports . handler = async ( event , context ) => { console . log ( 'name =' , event . name ); return event . name ; }; Let's set up webhook event-source to invoke the lambda over http requests. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Let's expose the webhook event-source using port-forward so that we can make a request to it. kubectl -n argo-events port-forward <name-of-event-source-pod> 12000:12000 Deploy the webhook sensor with AWS Lambda trigger kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/aws-lambda-trigger.yaml Once the sensor pod is in running state, make a curl request to webhook event-source pod, curl -d '{\"name\":\"foo\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example It will trigger the AWS Lambda function hello . Look at the CloudWatch logs to verify. Specification \u00b6 The AWS Lambda trigger specification is available here . Request Payload \u00b6 Invoking the AWS Lambda without a request payload would not be very useful. The lambda trigger within a sensor is invoked when sensor receives an event from the eventbus. In order to construct a request payload based on the event data, sensor offers payload field as a part of the lambda trigger. Let's examine a lambda trigger, awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . name dest : name The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"name\": \"foo\" // name field from event data } The above payload will be passed in the request to invoke the AWS lambda. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data. Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the AWS Lambda trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like function name, payload values on the fly. Consider a scenario where you don't want to hard-code the function name and let the event data populate it. awsLambda : functionName : hello // this will be replaced . accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message parameters : - src : dependencyName : test - dep dataKey : body . function_name dest : functionName With parameters the sensor will replace the function name hello with the value of field function_name from event data. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the lambda invocation and decide whether to stop or continue sensor. To determine whether the lamda was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message policy : status : allow : - 200 - 201 The above lambda trigger will be treated successful only if its invocation returns with either 200 or 201 status.","title":"AWS Lambda"},{"location":"triggers/aws-lambda/#aws-lambda","text":"AWS Lambda provides a tremendous value, but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS.","title":"AWS Lambda"},{"location":"triggers/aws-lambda/#trigger-a-simple-lambda","text":"Make sure to have eventbus deployed in the namespace. Make sure your AWS account has permissions to execute Lambda. More info on AWS permissions is available here . Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Create a basic lambda function called hello either using AWS cli or console. exports . handler = async ( event , context ) => { console . log ( 'name =' , event . name ); return event . name ; }; Let's set up webhook event-source to invoke the lambda over http requests. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Let's expose the webhook event-source using port-forward so that we can make a request to it. kubectl -n argo-events port-forward <name-of-event-source-pod> 12000:12000 Deploy the webhook sensor with AWS Lambda trigger kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/aws-lambda-trigger.yaml Once the sensor pod is in running state, make a curl request to webhook event-source pod, curl -d '{\"name\":\"foo\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example It will trigger the AWS Lambda function hello . Look at the CloudWatch logs to verify.","title":"Trigger A Simple Lambda"},{"location":"triggers/aws-lambda/#specification","text":"The AWS Lambda trigger specification is available here .","title":"Specification"},{"location":"triggers/aws-lambda/#request-payload","text":"Invoking the AWS Lambda without a request payload would not be very useful. The lambda trigger within a sensor is invoked when sensor receives an event from the eventbus. In order to construct a request payload based on the event data, sensor offers payload field as a part of the lambda trigger. Let's examine a lambda trigger, awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . name dest : name The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"name\": \"foo\" // name field from event data } The above payload will be passed in the request to invoke the AWS lambda. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data.","title":"Request Payload"},{"location":"triggers/aws-lambda/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the AWS Lambda trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like function name, payload values on the fly. Consider a scenario where you don't want to hard-code the function name and let the event data populate it. awsLambda : functionName : hello // this will be replaced . accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message parameters : - src : dependencyName : test - dep dataKey : body . function_name dest : functionName With parameters the sensor will replace the function name hello with the value of field function_name from event data. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/aws-lambda/#policy","text":"Trigger policy helps you determine the status of the lambda invocation and decide whether to stop or continue sensor. To determine whether the lamda was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message policy : status : allow : - 200 - 201 The above lambda trigger will be treated successful only if its invocation returns with either 200 or 201 status.","title":"Policy"},{"location":"triggers/build-your-own-trigger/","text":"Build Your Own Trigger \u00b6 Argo Events supports a variety of triggers out of box like Argo Workflow, K8s Objects, AWS Lambda, HTTP Requests etc., but you may want to write your own logic to trigger a pipeline or create an object in K8s cluster. An example would be to trigger TektonCD or AirFlow pipelines on GitHub events. Custom Trigger \u00b6 In order to plug your own implementation for a trigger with Argo Events Sensor, you need to run a gRPC server that implements the interface that the sensor expects. Interface \u00b6 The interface exposed via proto file, // Trigger offers services to build a custom trigger service Trigger { // FetchResource fetches the resource to be triggered. rpc FetchResource(FetchResourceRequest) returns (FetchResourceResponse); // Execute executes the requested trigger resource. rpc Execute(ExecuteRequest) returns (ExecuteResponse); // ApplyPolicy applies policies on the trigger execution result. rpc ApplyPolicy(ApplyPolicyRequest) returns (ApplyPolicyResponse); } The complete proto file is available here . Let's walk through the contract, FetchResource : If the trigger server needs to fetch a resource from external sources like S3, Git or a URL, this is the place to do so. e.g. if the trigger server aims to invoke a TektonCD pipeline and the PipelineRun resource lives on Git, then trigger server can first fetch it from Git and return it back to sensor. Execute : In this method, the trigger server executes/invokes the trigger. e.g. TektonCD pipeline resource being created in K8s cluster. ApplyPolicy : This is where your trigger implementation can check whether the triggered resource transitioned into the success state. Depending upon the response from the trigger server, the sensor will either stop processing subsequent triggers, or it will continue to process them. How to define the Custom Trigger in a sensor? \u00b6 Let's look at the following sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor spec : template : spec : containers : - name : sensor image : metalgearsolid / sensor : v0 .15.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : webhook eventName : example triggers : - template : name : webhook - workflow - trigger custom : # the url of the trigger server. serverURL : tekton - trigger . argo - events . svc : 9000 # spec is map of string->string and it is sent over to trigger server. # the spec can be anything you want as per your use-case, just make sure the trigger server understands the spec map. spec : url : \"https://raw.githubusercontent.com/VaibhavPage/tekton-cd-trigger/master/example.yaml\" # These parameters are applied on resource fetched and returned by the trigger server. # e.g. consider a trigger server which invokes TektonCD pipeline runs, then # the trigger server can return a TektonCD PipelineRun resource. # The parameters are then applied on that PipelineRun resource. parameters : - src : dependencyName : test - dep dataKey : body . namespace dest : metadata . namespace # These parameters are applied on entire template body. # So that you can parameterize anything under `custom` key such as `serverURL`, `spec` etc. parameters : - src : dependencyName : test - dep dataKey : body . url dest : custom . spec . url The sensor definition should look familiar to you. The only difference is the custom key under triggers -> template . The specification under custom key defines the custom trigger. The most important fields are, serverURL : This is the URL of the trigger gRPC server. spec : It is a map of string -> string. The spec can be anything you want as per your use-case. The sensor sends the spec to trigger server, and it is upto the trigger gRPC server to interpret the spec. parameters : The parameters override the resource that is fetched by the trigger server. Read more info on parameters here . payload : Payload to send to the trigger server. Read more on payload here . The complete spec for the custom trigger is available here . Custom Trigger in Action \u00b6 Refer to a sample trigger server that invokes TektonCD pipeline on events.","title":"Build Your Own Trigger"},{"location":"triggers/build-your-own-trigger/#build-your-own-trigger","text":"Argo Events supports a variety of triggers out of box like Argo Workflow, K8s Objects, AWS Lambda, HTTP Requests etc., but you may want to write your own logic to trigger a pipeline or create an object in K8s cluster. An example would be to trigger TektonCD or AirFlow pipelines on GitHub events.","title":"Build Your Own Trigger"},{"location":"triggers/build-your-own-trigger/#custom-trigger","text":"In order to plug your own implementation for a trigger with Argo Events Sensor, you need to run a gRPC server that implements the interface that the sensor expects.","title":"Custom Trigger"},{"location":"triggers/build-your-own-trigger/#interface","text":"The interface exposed via proto file, // Trigger offers services to build a custom trigger service Trigger { // FetchResource fetches the resource to be triggered. rpc FetchResource(FetchResourceRequest) returns (FetchResourceResponse); // Execute executes the requested trigger resource. rpc Execute(ExecuteRequest) returns (ExecuteResponse); // ApplyPolicy applies policies on the trigger execution result. rpc ApplyPolicy(ApplyPolicyRequest) returns (ApplyPolicyResponse); } The complete proto file is available here . Let's walk through the contract, FetchResource : If the trigger server needs to fetch a resource from external sources like S3, Git or a URL, this is the place to do so. e.g. if the trigger server aims to invoke a TektonCD pipeline and the PipelineRun resource lives on Git, then trigger server can first fetch it from Git and return it back to sensor. Execute : In this method, the trigger server executes/invokes the trigger. e.g. TektonCD pipeline resource being created in K8s cluster. ApplyPolicy : This is where your trigger implementation can check whether the triggered resource transitioned into the success state. Depending upon the response from the trigger server, the sensor will either stop processing subsequent triggers, or it will continue to process them.","title":"Interface"},{"location":"triggers/build-your-own-trigger/#how-to-define-the-custom-trigger-in-a-sensor","text":"Let's look at the following sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor spec : template : spec : containers : - name : sensor image : metalgearsolid / sensor : v0 .15.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : webhook eventName : example triggers : - template : name : webhook - workflow - trigger custom : # the url of the trigger server. serverURL : tekton - trigger . argo - events . svc : 9000 # spec is map of string->string and it is sent over to trigger server. # the spec can be anything you want as per your use-case, just make sure the trigger server understands the spec map. spec : url : \"https://raw.githubusercontent.com/VaibhavPage/tekton-cd-trigger/master/example.yaml\" # These parameters are applied on resource fetched and returned by the trigger server. # e.g. consider a trigger server which invokes TektonCD pipeline runs, then # the trigger server can return a TektonCD PipelineRun resource. # The parameters are then applied on that PipelineRun resource. parameters : - src : dependencyName : test - dep dataKey : body . namespace dest : metadata . namespace # These parameters are applied on entire template body. # So that you can parameterize anything under `custom` key such as `serverURL`, `spec` etc. parameters : - src : dependencyName : test - dep dataKey : body . url dest : custom . spec . url The sensor definition should look familiar to you. The only difference is the custom key under triggers -> template . The specification under custom key defines the custom trigger. The most important fields are, serverURL : This is the URL of the trigger gRPC server. spec : It is a map of string -> string. The spec can be anything you want as per your use-case. The sensor sends the spec to trigger server, and it is upto the trigger gRPC server to interpret the spec. parameters : The parameters override the resource that is fetched by the trigger server. Read more info on parameters here . payload : Payload to send to the trigger server. Read more on payload here . The complete spec for the custom trigger is available here .","title":"How to define the Custom Trigger in a sensor?"},{"location":"triggers/build-your-own-trigger/#custom-trigger-in-action","text":"Refer to a sample trigger server that invokes TektonCD pipeline on events.","title":"Custom Trigger in Action"},{"location":"triggers/http-trigger/","text":"HTTP Trigger \u00b6 Argo Events offers HTTP trigger which can easily invoke serverless functions like OpenFaas, Kubeless, Knative, Nuclio and make REST API calls. Specification \u00b6 The HTTP trigger specification is available here . REST API Calls \u00b6 Consider a scenario where your REST API server needs to consume events from event-sources S3, GitHub, SQS etc. Usually, you'd end up writing the integration yourself in the server code, although server logic has nothing to do any of the event-sources. This is where Argo Events HTTP trigger can help. The HTTP trigger takes the task of consuming events from event-sources away from API server and seamlessly integrates these events via REST API calls. We will set up a basic go http server and connect it with the minio events. The HTTP server simply prints the request body as follows, package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) func hello ( w http . ResponseWriter , req * http . Request ) { body , err := ioutil . ReadAll ( req . Body ) if err != nil { fmt . Printf ( \"%+v \\n \" , err ) return } fmt . Println ( string ( body )) fmt . Fprintf ( w , \"hello \\n \" ) } func main () { http . HandleFunc ( \"/hello\" , hello ) fmt . Println ( \"server is listening on 8090\" ) http . ListenAndServe ( \":8090\" , nil ) } Deploy the HTTP server, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/09-http-trigger/http-server.yaml Create a service to expose the http server kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/09-http-trigger/http-server-svc.yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server.. kubectl -n argo-events port-forward <http-server-pod-name> 8090:8090 Our goals is to seamlessly integrate Minio S3 bucket notifications with REST API server created in previous step. So, lets set up the Minio event-source available here . Don't create the sensor as we will be deploying it in next step. Create a sensor as follows, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/http-trigger.yaml Now, drop a file onto input bucket in Minio server. The sensor has triggered a http request to the http server. Take a look at the logs server is listening on 8090 {\"type\":\"minio\",\"bucket\":\"input\"} Great!!! Request Payload \u00b6 In order to construct a request payload based on the event data, sensor offers payload field as a part of the HTP trigger. Let's examine a HTTP trigger, http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0. s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"type\": \"type of event from event's context\" \"bucket\": \"bucket name from event data\" } The above payload will be passed in the HTTP request. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data. Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the HTTP trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like URL, payload values on the fly. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the HTTP request and decide whether to stop or continue sensor. To determine whether the HTTP request was successful or not, the HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0 s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . policy : status : allow : - 200 - 201 The above HTTP trigger will be treated successful only if the HTTP request returns with either 200 or 201 status. OpenFaas \u00b6 OpenFaas offers a simple way to spin up serverless functions. Lets see how we can leverage Argo Events HTTP trigger to invoke OpenFaas function. If you don't have OpenFaas installed, follow the instructions . Let's create a basic function. You can follow the steps to set up the function. package function import ( \"fmt\" ) // Handle a serverless request func Handle ( req [] byte ) string { return fmt . Sprintf ( \"Hello, Go. You said: %s \" , string ( req )) } Make sure the function pod is up and running. We are going to invoke OpenFaas function on a message on Redis Subscriber. Let's set up the Redis Database, Redis PubSub event-source as specified here . Do not create the Redis sensor, we are going to create it in next step. Let's create the sensor with OpenFaas trigger apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : redis - sensor spec : template : serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : redis eventName : example triggers : - template : name : openfaas - trigger http : url : http : // gateway . openfaas . svc . cluster . local : 8080 / function / gohash payload : - src : dependencyName : test - dep dest : bucket method : POST Publish a message on FOO channel using redis-cli . PUBLISH FOO hello As soon as you publish the message, the sensor will invoke the OpenFaas function gohash . Kubeless \u00b6 Similar to REST API calls, you can easily invoke Kubeless functions using HTTP trigger. If you don't have Kubeless installed, follow the installation . Lets create a basic function, def hello ( event , context ) : print event return event [ ' data ' ] Make sure the function pod and service is created. Now, we are going to invoke the Kubeless function when a message is placed on a NATS queue. Let's set up the NATS event-source. Follow instructions for details. Do not create the NATS sensor, we are going to create it in next step. Let's create NATS sensor with HTTP trigger, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : nats - sensor spec : template : serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : nats eventName : example triggers : - template : name : kubeless - trigger http : serverURL : http : // hello . kubeless . svc . cluster . local : 8080 payload : - src : dependencyName : test - dep dataKey : body . first_name dest : first_name - src : dependencyName : test - dep dataKey : body . last_name dest : last_name method : POST Once event-source and sensor pod are up and running, dispatch a message on foo subject using nats client, go run main . go - s localhost foo '{\"first_name\": \"foo\", \"last_name\": \"bar\"}' It will invoke Kubeless function hello , {'event-time': None, 'extensions': {'request': <LocalRequest: POST http://hello.kubeless.svc.cluster.local:8080/> }, 'event-type': None, 'event-namespace': None, 'data': '{\"first_name\":\"foo\",\"last_name\":\"bar\"}', 'event-id': None} Other serverless frameworks \u00b6 Similar to OpenFaas and Kubeless invocation demonstrated above, you can easily trigger KNative, Nuclio, Fission functions using HTTP trigger.","title":"HTTP Trigger"},{"location":"triggers/http-trigger/#http-trigger","text":"Argo Events offers HTTP trigger which can easily invoke serverless functions like OpenFaas, Kubeless, Knative, Nuclio and make REST API calls.","title":"HTTP Trigger"},{"location":"triggers/http-trigger/#specification","text":"The HTTP trigger specification is available here .","title":"Specification"},{"location":"triggers/http-trigger/#rest-api-calls","text":"Consider a scenario where your REST API server needs to consume events from event-sources S3, GitHub, SQS etc. Usually, you'd end up writing the integration yourself in the server code, although server logic has nothing to do any of the event-sources. This is where Argo Events HTTP trigger can help. The HTTP trigger takes the task of consuming events from event-sources away from API server and seamlessly integrates these events via REST API calls. We will set up a basic go http server and connect it with the minio events. The HTTP server simply prints the request body as follows, package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) func hello ( w http . ResponseWriter , req * http . Request ) { body , err := ioutil . ReadAll ( req . Body ) if err != nil { fmt . Printf ( \"%+v \\n \" , err ) return } fmt . Println ( string ( body )) fmt . Fprintf ( w , \"hello \\n \" ) } func main () { http . HandleFunc ( \"/hello\" , hello ) fmt . Println ( \"server is listening on 8090\" ) http . ListenAndServe ( \":8090\" , nil ) } Deploy the HTTP server, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/09-http-trigger/http-server.yaml Create a service to expose the http server kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/09-http-trigger/http-server-svc.yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server.. kubectl -n argo-events port-forward <http-server-pod-name> 8090:8090 Our goals is to seamlessly integrate Minio S3 bucket notifications with REST API server created in previous step. So, lets set up the Minio event-source available here . Don't create the sensor as we will be deploying it in next step. Create a sensor as follows, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/http-trigger.yaml Now, drop a file onto input bucket in Minio server. The sensor has triggered a http request to the http server. Take a look at the logs server is listening on 8090 {\"type\":\"minio\",\"bucket\":\"input\"} Great!!!","title":"REST API Calls"},{"location":"triggers/http-trigger/#request-payload","text":"In order to construct a request payload based on the event data, sensor offers payload field as a part of the HTP trigger. Let's examine a HTTP trigger, http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0. s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"type\": \"type of event from event's context\" \"bucket\": \"bucket name from event data\" } The above payload will be passed in the HTTP request. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data.","title":"Request Payload"},{"location":"triggers/http-trigger/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the HTTP trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like URL, payload values on the fly. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/http-trigger/#policy","text":"Trigger policy helps you determine the status of the HTTP request and decide whether to stop or continue sensor. To determine whether the HTTP request was successful or not, the HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0 s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . policy : status : allow : - 200 - 201 The above HTTP trigger will be treated successful only if the HTTP request returns with either 200 or 201 status.","title":"Policy"},{"location":"triggers/http-trigger/#openfaas","text":"OpenFaas offers a simple way to spin up serverless functions. Lets see how we can leverage Argo Events HTTP trigger to invoke OpenFaas function. If you don't have OpenFaas installed, follow the instructions . Let's create a basic function. You can follow the steps to set up the function. package function import ( \"fmt\" ) // Handle a serverless request func Handle ( req [] byte ) string { return fmt . Sprintf ( \"Hello, Go. You said: %s \" , string ( req )) } Make sure the function pod is up and running. We are going to invoke OpenFaas function on a message on Redis Subscriber. Let's set up the Redis Database, Redis PubSub event-source as specified here . Do not create the Redis sensor, we are going to create it in next step. Let's create the sensor with OpenFaas trigger apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : redis - sensor spec : template : serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : redis eventName : example triggers : - template : name : openfaas - trigger http : url : http : // gateway . openfaas . svc . cluster . local : 8080 / function / gohash payload : - src : dependencyName : test - dep dest : bucket method : POST Publish a message on FOO channel using redis-cli . PUBLISH FOO hello As soon as you publish the message, the sensor will invoke the OpenFaas function gohash .","title":"OpenFaas"},{"location":"triggers/http-trigger/#kubeless","text":"Similar to REST API calls, you can easily invoke Kubeless functions using HTTP trigger. If you don't have Kubeless installed, follow the installation . Lets create a basic function, def hello ( event , context ) : print event return event [ ' data ' ] Make sure the function pod and service is created. Now, we are going to invoke the Kubeless function when a message is placed on a NATS queue. Let's set up the NATS event-source. Follow instructions for details. Do not create the NATS sensor, we are going to create it in next step. Let's create NATS sensor with HTTP trigger, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : nats - sensor spec : template : serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : nats eventName : example triggers : - template : name : kubeless - trigger http : serverURL : http : // hello . kubeless . svc . cluster . local : 8080 payload : - src : dependencyName : test - dep dataKey : body . first_name dest : first_name - src : dependencyName : test - dep dataKey : body . last_name dest : last_name method : POST Once event-source and sensor pod are up and running, dispatch a message on foo subject using nats client, go run main . go - s localhost foo '{\"first_name\": \"foo\", \"last_name\": \"bar\"}' It will invoke Kubeless function hello , {'event-time': None, 'extensions': {'request': <LocalRequest: POST http://hello.kubeless.svc.cluster.local:8080/> }, 'event-type': None, 'event-namespace': None, 'data': '{\"first_name\":\"foo\",\"last_name\":\"bar\"}', 'event-id': None}","title":"Kubeless"},{"location":"triggers/http-trigger/#other-serverless-frameworks","text":"Similar to OpenFaas and Kubeless invocation demonstrated above, you can easily trigger KNative, Nuclio, Fission functions using HTTP trigger.","title":"Other serverless frameworks"},{"location":"triggers/k8s-object-trigger/","text":"Kubernetes Object Trigger \u00b6 Apart from Argo workflow objects, the sensor lets you trigger standard Kubernetes objects such as Pod, Deployment, Job, CronJob, etc. Having the ability to trigger standard Kubernetes objects is quite powerful as provides an avenue to set up event-driven pipelines for existing workloads. Trigger a K8s Pod \u00b6 We will use webhook event-source and sensor to trigger a K8s pod. Lets set up a webhook event source to process incoming requests. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml To trigger a pod, we need to create a sensor as defined below, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook spec : template : serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : webhook eventName : example triggers : - template : name : webhook - pod - trigger k8s : group : \"\" version : v1 resource : pods operation : create source : resource : apiVersion : v1 kind : Pod metadata : generateName : hello - world - spec : containers : - name : hello - container args : - \"hello-world\" command : - cowsay image : \"docker/whalesay:latest\" parameters : - src : dependencyName : test - dep dest : spec . containers . 0 . args . 0 The group , version and resource under k8s in the trigger template determines the type of K8s object. Change it accordingly if you want to trigger something else than a pod. Create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/trigger-standard-k8s-resource.yaml Lets expose the webhook event-source pod using port-forward so that we can make a request to it. kubectl -n argo-events port-forward <name-of-event-source-pod> 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the logs of the pod, you will something similar as below, _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\",\"time\":\"2020-01-11T21:23:07.682961 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Operation \u00b6 You can specify the operation for the trigger using the operation key under triggers->template->k8s. Operation can be either, create : Creates the object if not available in K8s cluster. update : Updates the object. patch : Patches the object using given patch strategy. More info available at here . Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the K8s trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the K8s object values on the fly. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the triggered K8s object and decide whether to stop or continue sensor. To determine whether the K8s object was successful or not, the K8s trigger provides a Resource Labels policy. The Resource Labels holds a list of labels which are checked against the triggered K8s object to determine the status of the object. # Policy to configure backoff and execution criteria for the trigger # Because the sensor is able to trigger any K8s resource , it determines the resource state by looking at the resource ' s labels. policy : k8s : # Backoff before checking the resource labels backoff : # Duration is the duration in nanoseconds duration : 1000000000 # 1 second # Duration is multiplied by factor each iteration factor : 2 # The amount of jitter applied each iteration jitter : 0 . 1 # Exit with error after these many steps steps : 5 # labels set on the resource decide if the resource has transitioned into the success state . labels : workflows . argoproj . io / phase : Succeeded # Determines whether trigger should be marked as failed if the backoff times out and sensor is still unable to decide the state of the trigger . # defaults to false errorOnBackoffTimeout : true Complete example is available here .","title":"Kubernetes Object Trigger"},{"location":"triggers/k8s-object-trigger/#kubernetes-object-trigger","text":"Apart from Argo workflow objects, the sensor lets you trigger standard Kubernetes objects such as Pod, Deployment, Job, CronJob, etc. Having the ability to trigger standard Kubernetes objects is quite powerful as provides an avenue to set up event-driven pipelines for existing workloads.","title":"Kubernetes Object Trigger"},{"location":"triggers/k8s-object-trigger/#trigger-a-k8s-pod","text":"We will use webhook event-source and sensor to trigger a K8s pod. Lets set up a webhook event source to process incoming requests. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml To trigger a pod, we need to create a sensor as defined below, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook spec : template : serviceAccountName : argo - events - sa dependencies : - name : test - dep eventSourceName : webhook eventName : example triggers : - template : name : webhook - pod - trigger k8s : group : \"\" version : v1 resource : pods operation : create source : resource : apiVersion : v1 kind : Pod metadata : generateName : hello - world - spec : containers : - name : hello - container args : - \"hello-world\" command : - cowsay image : \"docker/whalesay:latest\" parameters : - src : dependencyName : test - dep dest : spec . containers . 0 . args . 0 The group , version and resource under k8s in the trigger template determines the type of K8s object. Change it accordingly if you want to trigger something else than a pod. Create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/trigger-standard-k8s-resource.yaml Lets expose the webhook event-source pod using port-forward so that we can make a request to it. kubectl -n argo-events port-forward <name-of-event-source-pod> 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the logs of the pod, you will something similar as below, _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\",\"time\":\"2020-01-11T21:23:07.682961 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Trigger a K8s Pod"},{"location":"triggers/k8s-object-trigger/#operation","text":"You can specify the operation for the trigger using the operation key under triggers->template->k8s. Operation can be either, create : Creates the object if not available in K8s cluster. update : Updates the object. patch : Patches the object using given patch strategy. More info available at here .","title":"Operation"},{"location":"triggers/k8s-object-trigger/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the K8s trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the K8s object values on the fly. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/k8s-object-trigger/#policy","text":"Trigger policy helps you determine the status of the triggered K8s object and decide whether to stop or continue sensor. To determine whether the K8s object was successful or not, the K8s trigger provides a Resource Labels policy. The Resource Labels holds a list of labels which are checked against the triggered K8s object to determine the status of the object. # Policy to configure backoff and execution criteria for the trigger # Because the sensor is able to trigger any K8s resource , it determines the resource state by looking at the resource ' s labels. policy : k8s : # Backoff before checking the resource labels backoff : # Duration is the duration in nanoseconds duration : 1000000000 # 1 second # Duration is multiplied by factor each iteration factor : 2 # The amount of jitter applied each iteration jitter : 0 . 1 # Exit with error after these many steps steps : 5 # labels set on the resource decide if the resource has transitioned into the success state . labels : workflows . argoproj . io / phase : Succeeded # Determines whether trigger should be marked as failed if the backoff times out and sensor is still unable to decide the state of the trigger . # defaults to false errorOnBackoffTimeout : true Complete example is available here .","title":"Policy"},{"location":"triggers/kafka-trigger/","text":"Kafka Trigger \u00b6 Kafka trigger allows sensor to publish events on Kafka topic. This trigger helps source the events from outside world into your messaging queues. Specification \u00b6 The Kafka trigger specification is available here . Walkthrough \u00b6 Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a Kafka topic. Set up the Minio Event Source here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor spec : dependencies : - name : test - dep eventSourceName : minio eventName : example triggers : - template : name : kafka - trigger kafka : # Kafka URL url : kafka . argo - events . svc : 9092 # Name of the topic topic : minio - events # partition id partition : 0 payload : - src : dependencyName : test - dep dataKey : notification . 0. s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0. s3 . bucket . name dest : bucket The Kafka message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the Kafka trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\": \"hello.txt\" // name/key of the object \"bucket\": \"input\" // name of the bucket } Drop a file called hello.txt onto the bucket input and you will receive the message on Kafka topic","title":"Kafka Trigger"},{"location":"triggers/kafka-trigger/#kafka-trigger","text":"Kafka trigger allows sensor to publish events on Kafka topic. This trigger helps source the events from outside world into your messaging queues.","title":"Kafka Trigger"},{"location":"triggers/kafka-trigger/#specification","text":"The Kafka trigger specification is available here .","title":"Specification"},{"location":"triggers/kafka-trigger/#walkthrough","text":"Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a Kafka topic. Set up the Minio Event Source here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor spec : dependencies : - name : test - dep eventSourceName : minio eventName : example triggers : - template : name : kafka - trigger kafka : # Kafka URL url : kafka . argo - events . svc : 9092 # Name of the topic topic : minio - events # partition id partition : 0 payload : - src : dependencyName : test - dep dataKey : notification . 0. s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0. s3 . bucket . name dest : bucket The Kafka message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the Kafka trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\": \"hello.txt\" // name/key of the object \"bucket\": \"input\" // name of the bucket } Drop a file called hello.txt onto the bucket input and you will receive the message on Kafka topic","title":"Walkthrough"},{"location":"triggers/log/","text":"Log \u00b6 Log trigger is for debugging - it just logs events it receives as JSON: { \"level\" : \"info\" , \"ts\" : 1604783266.973979 , \"logger\" : \"argo-events.sensor\" , \"caller\" : \"log/log.go:35\" , \"msg\" : \"{\\\"eventTime\\\":\\\"2020-11-07 21:07:46.9658533 +0000 UTC m=+20468.986115001\\\"}\" , \"sensorName\" : \"log\" , \"triggerName\" : \"log-trigger\" , \"dependencyName\" : \"test-dep\" , \"eventContext\" : \"{\\\"id\\\":\\\"37363664356662642d616364322d343563332d396362622d353037653361343637393237\\\",\\\"source\\\":\\\"calendar\\\",\\\"specversion\\\":\\\"1.0\\\",\\\"type\\\":\\\"calendar\\\",\\\"datacontenttype\\\":\\\"application/json\\\",\\\"subject\\\":\\\"example-with-interval\\\",\\\"time\\\":\\\"2020-11-07T21:07:46Z\\\"}\" } Specification \u00b6 The specification is available here . Parameterization \u00b6 No parameterization is supported.","title":"Log"},{"location":"triggers/log/#log","text":"Log trigger is for debugging - it just logs events it receives as JSON: { \"level\" : \"info\" , \"ts\" : 1604783266.973979 , \"logger\" : \"argo-events.sensor\" , \"caller\" : \"log/log.go:35\" , \"msg\" : \"{\\\"eventTime\\\":\\\"2020-11-07 21:07:46.9658533 +0000 UTC m=+20468.986115001\\\"}\" , \"sensorName\" : \"log\" , \"triggerName\" : \"log-trigger\" , \"dependencyName\" : \"test-dep\" , \"eventContext\" : \"{\\\"id\\\":\\\"37363664356662642d616364322d343563332d396362622d353037653361343637393237\\\",\\\"source\\\":\\\"calendar\\\",\\\"specversion\\\":\\\"1.0\\\",\\\"type\\\":\\\"calendar\\\",\\\"datacontenttype\\\":\\\"application/json\\\",\\\"subject\\\":\\\"example-with-interval\\\",\\\"time\\\":\\\"2020-11-07T21:07:46Z\\\"}\" }","title":"Log"},{"location":"triggers/log/#specification","text":"The specification is available here .","title":"Specification"},{"location":"triggers/log/#parameterization","text":"No parameterization is supported.","title":"Parameterization"},{"location":"triggers/nats-trigger/","text":"NATS Trigger \u00b6 NATS trigger allows sensor to publish events on NATS subjects. This trigger helps source the events from outside world into your messaging queues. Specification \u00b6 The NATS trigger specification is available here . Walkthrough \u00b6 Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a NATS subject. Set up the Minio Event Source here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor spec : dependencies : - name : test - dep eventSourceName : minio eventName : example triggers : - template : name : nats - trigger nats : # NATS Server URL url : nats . argo - events . svc : 4222 # Name of the subject subject : minio - events payload : - src : dependencyName : test - dep dataKey : notification . 0. s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0. s3 . bucket . name dest : bucket The NATS message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the NATS trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\": \"hello.txt\" // name/key of the object \"bucket\": \"input\" // name of the bucket } If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl -n argo-events port-forward <nats-pod-name> 4222:4222 Subscribe to the subject called minio-events . Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost minio - events ' Drop a file called hello.txt onto the bucket input and you will receive the message on NATS subscriber as follows, [#1] Received on [minio-events]: '{\"bucket\":\"input\",\"fileName\":\"hello.txt\"}'","title":"NATS Trigger"},{"location":"triggers/nats-trigger/#nats-trigger","text":"NATS trigger allows sensor to publish events on NATS subjects. This trigger helps source the events from outside world into your messaging queues.","title":"NATS Trigger"},{"location":"triggers/nats-trigger/#specification","text":"The NATS trigger specification is available here .","title":"Specification"},{"location":"triggers/nats-trigger/#walkthrough","text":"Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a NATS subject. Set up the Minio Event Source here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor spec : dependencies : - name : test - dep eventSourceName : minio eventName : example triggers : - template : name : nats - trigger nats : # NATS Server URL url : nats . argo - events . svc : 4222 # Name of the subject subject : minio - events payload : - src : dependencyName : test - dep dataKey : notification . 0. s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0. s3 . bucket . name dest : bucket The NATS message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the NATS trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\": \"hello.txt\" // name/key of the object \"bucket\": \"input\" // name of the bucket } If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl -n argo-events port-forward <nats-pod-name> 4222:4222 Subscribe to the subject called minio-events . Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost minio - events ' Drop a file called hello.txt onto the bucket input and you will receive the message on NATS subscriber as follows, [#1] Received on [minio-events]: '{\"bucket\":\"input\",\"fileName\":\"hello.txt\"}'","title":"Walkthrough"},{"location":"triggers/openwhisk-trigger/","text":"OpenWhisk Trigger \u00b6 OpenWhisk is a framework to run serverless workloads. It ships with its own event sources but their numbers are limited and it doesn't have support for circuits, parameterization, filtering, on-demand payload construction, etc that a sensor provides. Prerequisite \u00b6 OpenWhisk must be up and running. Setup \u00b6 Coming Soon...","title":"OpenWhisk Trigger"},{"location":"triggers/openwhisk-trigger/#openwhisk-trigger","text":"OpenWhisk is a framework to run serverless workloads. It ships with its own event sources but their numbers are limited and it doesn't have support for circuits, parameterization, filtering, on-demand payload construction, etc that a sensor provides.","title":"OpenWhisk Trigger"},{"location":"triggers/openwhisk-trigger/#prerequisite","text":"OpenWhisk must be up and running.","title":"Prerequisite"},{"location":"triggers/openwhisk-trigger/#setup","text":"Coming Soon...","title":"Setup"},{"location":"triggers/slack-trigger/","text":"Slack Trigger \u00b6 The Slack trigger is used to send a custom message to a desired Slack channel in a Slack workspace. The intended use is for notifications for a build pipeline, but can be used for any notification scenario. Prerequisite \u00b6 Deploy the eventbus in the namespace. Make sure to have a Slack workspace setup you wish to send a message to. Create a webhook event-source. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Set up port-forwarding to expose the http server. We will use port-forwarding here. kubectl port-forward -n argo-events <event-source-pod-name> 12000:12000 Create a Slack App \u00b6 We need to create a Slack App which will send messages to your Slack Workspace. We will add OAuth Permissions and add the OAuth token to the k8s cluster via a secret. Create a Slack app by clicking Create New App at the Slack API Page . Name your app and choose your intended Slack Workspace Navigate to your app, then to Features > OAuth & Permissions Scroll down to Scopes and add the scopes channels:join , channels:read and chat:write to the Bot Token Scopes Scroll to the top of the OAuth & Permissions page and click Install App to Workspace and follow the install Wizard You should land back on the OAuth & Permissions page. Copy your app's OAuth Access Token. This will allow the trigger to act on behalf of your newly created Slack app. Encode your OAuth token in base64. This can done easily with the command line echo -n \"YOUR-OAUTH-TOKEN\" | base64 Create a kubernetes secret file slack-secret.yaml with your OAuth token in the following format apiVersion : v1 kind : Secret metadata : name : slack - secret data : token : YOUR - BASE64 - ENCODED - OAUTH - TOKEN Apply the kubernetes secret kubectl -n argo-events apply -f slack-secret.yaml Slack Trigger \u00b6 We will set up a basic slack trigger and send a default message, and then a dynamic custom message. Create a sensor with Slack trigger. We will discuss the trigger details in the following sections. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/slack-trigger.yaml Send a http request to the event-source-pod to fire the Slack trigger. curl -d '{\"text\":\"Hello, World!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Note : The default slack-trigger will send the message \"hello world\" to the #general channel. You may change the default message and channel in slack-trigger.yaml under triggers.slack.channel and triggers.slack.message. Alternatively, you can dynamically determine the channel and message based on parameterization of your event. curl - d ' {\"channel\":\"random\",\"message\":\"test message\"} ' - H \" Content-Type: application/json \" - X POST http : // localhost : 12000 / example Great! But, how did the sensor use the event to customize the message and channel from the http request? We will see that in next section. Parameterization \u00b6 The slack trigger parameters have the following structure, parameters: - src: dependencyName: test-dep dataKey: body.channel dest: slack.channel - src: dependencyName: test-dep contextKey: body.message dest: slack.message The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger paramters will generate a request payload as, { \"channel\": \"channel_to_send_message\", \"message\": \"message_to_send_to_channel\" } Note : If you define both the contextKey and dataKey within a paramter item, then the dataKey takes the precedence. You can create any paramater structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of Slack trigger is available here .","title":"Slack Trigger"},{"location":"triggers/slack-trigger/#slack-trigger","text":"The Slack trigger is used to send a custom message to a desired Slack channel in a Slack workspace. The intended use is for notifications for a build pipeline, but can be used for any notification scenario.","title":"Slack Trigger"},{"location":"triggers/slack-trigger/#prerequisite","text":"Deploy the eventbus in the namespace. Make sure to have a Slack workspace setup you wish to send a message to. Create a webhook event-source. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Set up port-forwarding to expose the http server. We will use port-forwarding here. kubectl port-forward -n argo-events <event-source-pod-name> 12000:12000","title":"Prerequisite"},{"location":"triggers/slack-trigger/#create-a-slack-app","text":"We need to create a Slack App which will send messages to your Slack Workspace. We will add OAuth Permissions and add the OAuth token to the k8s cluster via a secret. Create a Slack app by clicking Create New App at the Slack API Page . Name your app and choose your intended Slack Workspace Navigate to your app, then to Features > OAuth & Permissions Scroll down to Scopes and add the scopes channels:join , channels:read and chat:write to the Bot Token Scopes Scroll to the top of the OAuth & Permissions page and click Install App to Workspace and follow the install Wizard You should land back on the OAuth & Permissions page. Copy your app's OAuth Access Token. This will allow the trigger to act on behalf of your newly created Slack app. Encode your OAuth token in base64. This can done easily with the command line echo -n \"YOUR-OAUTH-TOKEN\" | base64 Create a kubernetes secret file slack-secret.yaml with your OAuth token in the following format apiVersion : v1 kind : Secret metadata : name : slack - secret data : token : YOUR - BASE64 - ENCODED - OAUTH - TOKEN Apply the kubernetes secret kubectl -n argo-events apply -f slack-secret.yaml","title":"Create a Slack App"},{"location":"triggers/slack-trigger/#slack-trigger_1","text":"We will set up a basic slack trigger and send a default message, and then a dynamic custom message. Create a sensor with Slack trigger. We will discuss the trigger details in the following sections. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/slack-trigger.yaml Send a http request to the event-source-pod to fire the Slack trigger. curl -d '{\"text\":\"Hello, World!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Note : The default slack-trigger will send the message \"hello world\" to the #general channel. You may change the default message and channel in slack-trigger.yaml under triggers.slack.channel and triggers.slack.message. Alternatively, you can dynamically determine the channel and message based on parameterization of your event. curl - d ' {\"channel\":\"random\",\"message\":\"test message\"} ' - H \" Content-Type: application/json \" - X POST http : // localhost : 12000 / example Great! But, how did the sensor use the event to customize the message and channel from the http request? We will see that in next section.","title":"Slack Trigger"},{"location":"triggers/slack-trigger/#parameterization","text":"The slack trigger parameters have the following structure, parameters: - src: dependencyName: test-dep dataKey: body.channel dest: slack.channel - src: dependencyName: test-dep contextKey: body.message dest: slack.message The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger paramters will generate a request payload as, { \"channel\": \"channel_to_send_message\", \"message\": \"message_to_send_to_channel\" } Note : If you define both the contextKey and dataKey within a paramter item, then the dataKey takes the precedence. You can create any paramater structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of Slack trigger is available here .","title":"Parameterization"},{"location":"tutorials/01-introduction/","text":"Introduction \u00b6 In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of event-source. Prerequisites \u00b6 Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind eventbus , sensor , event source . Get Started \u00b6 We are going to set up a sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Let' set up the eventbus, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Create the webhook event source. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Create the webhook sensor. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml If the commands are executed successfully, the eventbus, event-source and sensor pods will get created. You will also notice that a service is created for the event-source. Expose the event-source pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl -n argo-events port-forward <event-source-pod-name> 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf Make sure the workflow pod ran successfully. _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337\",\"time\":\"2020-01-11T16:55:42.996636 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0=\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments. Troubleshoot \u00b6 If you don't see the event-source and sensor pod in argo-events namespace, Inspect the event-source kubectl -n argo-events get eventsource event-source-object-name -o yaml Inspect the sensor, kubectl -n argo-events get sensor sensor-object-name -o yaml and look for any errors within the Status . 2. Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both event-source and sensor controller. 3. Check the logs of event-source and sensor controller. Make sure the controllers have processed the event-source and sensor objects and there are no errors. 4. Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Introduction"},{"location":"tutorials/01-introduction/#introduction","text":"In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of event-source.","title":"Introduction"},{"location":"tutorials/01-introduction/#prerequisites","text":"Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind eventbus , sensor , event source .","title":"Prerequisites"},{"location":"tutorials/01-introduction/#get-started","text":"We are going to set up a sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Let' set up the eventbus, kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Create the webhook event source. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml Create the webhook sensor. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml If the commands are executed successfully, the eventbus, event-source and sensor pods will get created. You will also notice that a service is created for the event-source. Expose the event-source pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl -n argo-events port-forward <event-source-pod-name> 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf Make sure the workflow pod ran successfully. _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337\",\"time\":\"2020-01-11T16:55:42.996636 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0=\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments.","title":"Get Started"},{"location":"tutorials/01-introduction/#troubleshoot","text":"If you don't see the event-source and sensor pod in argo-events namespace, Inspect the event-source kubectl -n argo-events get eventsource event-source-object-name -o yaml Inspect the sensor, kubectl -n argo-events get sensor sensor-object-name -o yaml and look for any errors within the Status . 2. Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both event-source and sensor controller. 3. Check the logs of event-source and sensor controller. Make sure the controllers have processed the event-source and sensor objects and there are no errors. 4. Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Troubleshoot"},{"location":"tutorials/02-parameterization/","text":"Parameterization \u00b6 In the previous section, we saw how to set up a basic webhook event-source and sensor. The trigger template had parameters set in the sensor object, and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload. Trigger Resource Parameterization \u00b6 If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In the previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload. Prerequisites \u00b6 Make sure to have the basic webhook event-source and sensor set up. Follow the introduction tutorial if haven't done already. Webhook Event Payload \u00b6 Webhook event-source consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"header\": {}, \"body\": {}, } } Context : This is the CloudEvent context and it is populated by the event-source regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the event-source. The event-source extracts the headers from the request and put it in the header within event data . Body : This is the request payload from the HTTP request. Event Context \u00b6 Now that we have an understanding of the structure of the event the webhook sensor receives from the event-source over the eventbus, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-01.yaml Send a HTTP request to the event-source pod. curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ < webhook > --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type . Event Data \u00b6 Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-02.yaml Send a HTTP request to the event-source pod. curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence. Default Values \u00b6 Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-03.yaml Send a HTTP request to the event-source pod. curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ < wow! a default value. > ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Sprig Templates \u00b6 The sprig template exposed through contextTemplate and dataTemplate lets you alter the event context and event data before it gets applied to the trigger via parameters . Take a look at the example defined here , it contains the parameters as follows, parameters : # Retrieve the 'message' key from the payload - src : dependencyName : test - dep dataTemplate : \"{{ .Input.body.message | title }}\" dest : spec . arguments . parameters . 0. value # Title case the context subject - src : dependencyName : test - dep contextTemplate : \"{{ .Input.subject | title }}\" dest : spec . arguments . parameters . 1. value # Retrieve the 'name' key from the payload, remove all whitespace and lowercase it. - src : dependencyName : test - dep dataTemplate : \"{{ .Input.body.name | nospace | lower }}-\" dest : metadata . generateName operation : append Consider the event the sensor received has format like, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": { \"name\": \"foo bar\", \"message\": \"hello there!!\" }, } } The parameters are transformed as, The first parameter extracts the body.message from event data and applies title filter which basically capitalizes the first letter and replaces the spec.arguments.parameters.0.value . The second parameter extracts the subject from the event context and again applies title filter and replaces the spec.arguments.parameters.1.value . The third parameter extracts the body.name from the event data, applies nospace filter which removes all white spaces and then lower filter which lowercases the text and appends it to metadata.generateName . Send a curl request to event-source as follows, curl -d '{\"name\":\"foo bar\", \"message\": \"hello there!!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example and you will see an Argo workflow being sprung with name like webhook-foobar-xxxxx . Check the output of the workflow, it should print something like, ____________________________ < Hello There!! from Example > ---------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Operations \u00b6 Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-04.yaml Send a HTTP request to the event-source. curl -d '{\"message\":\"hey!!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ < hey!!hello world > ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Trigger Template Parameterization \u00b6 The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-05.yaml Send a HTTP request to the event-source. curl -d '{\"dependencyName\":\"test-dep\", \"dataKey\": \"body.message\", \"dest\": \"spec.arguments.parameters.0.value\", \"message\": \"amazing!!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ < amazing!! > ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#parameterization","text":"In the previous section, we saw how to set up a basic webhook event-source and sensor. The trigger template had parameters set in the sensor object, and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#trigger-resource-parameterization","text":"If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In the previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload.","title":"Trigger Resource Parameterization"},{"location":"tutorials/02-parameterization/#prerequisites","text":"Make sure to have the basic webhook event-source and sensor set up. Follow the introduction tutorial if haven't done already.","title":"Prerequisites"},{"location":"tutorials/02-parameterization/#webhook-event-payload","text":"Webhook event-source consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the event-source over the eventbus looks like following, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"header\": {}, \"body\": {}, } } Context : This is the CloudEvent context and it is populated by the event-source regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the event-source. The event-source extracts the headers from the request and put it in the header within event data . Body : This is the request payload from the HTTP request.","title":"Webhook Event Payload"},{"location":"tutorials/02-parameterization/#event-context","text":"Now that we have an understanding of the structure of the event the webhook sensor receives from the event-source over the eventbus, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-01.yaml Send a HTTP request to the event-source pod. curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ < webhook > --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type .","title":"Event Context"},{"location":"tutorials/02-parameterization/#event-data","text":"Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-02.yaml Send a HTTP request to the event-source pod. curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence.","title":"Event Data"},{"location":"tutorials/02-parameterization/#default-values","text":"Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-03.yaml Send a HTTP request to the event-source pod. curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ < wow! a default value. > ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Default Values"},{"location":"tutorials/02-parameterization/#sprig-templates","text":"The sprig template exposed through contextTemplate and dataTemplate lets you alter the event context and event data before it gets applied to the trigger via parameters . Take a look at the example defined here , it contains the parameters as follows, parameters : # Retrieve the 'message' key from the payload - src : dependencyName : test - dep dataTemplate : \"{{ .Input.body.message | title }}\" dest : spec . arguments . parameters . 0. value # Title case the context subject - src : dependencyName : test - dep contextTemplate : \"{{ .Input.subject | title }}\" dest : spec . arguments . parameters . 1. value # Retrieve the 'name' key from the payload, remove all whitespace and lowercase it. - src : dependencyName : test - dep dataTemplate : \"{{ .Input.body.name | nospace | lower }}-\" dest : metadata . generateName operation : append Consider the event the sensor received has format like, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"body\": { \"name\": \"foo bar\", \"message\": \"hello there!!\" }, } } The parameters are transformed as, The first parameter extracts the body.message from event data and applies title filter which basically capitalizes the first letter and replaces the spec.arguments.parameters.0.value . The second parameter extracts the subject from the event context and again applies title filter and replaces the spec.arguments.parameters.1.value . The third parameter extracts the body.name from the event data, applies nospace filter which removes all white spaces and then lower filter which lowercases the text and appends it to metadata.generateName . Send a curl request to event-source as follows, curl -d '{\"name\":\"foo bar\", \"message\": \"hello there!!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example and you will see an Argo workflow being sprung with name like webhook-foobar-xxxxx . Check the output of the workflow, it should print something like, ____________________________ < Hello There!! from Example > ---------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Sprig Templates"},{"location":"tutorials/02-parameterization/#operations","text":"Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-04.yaml Send a HTTP request to the event-source. curl -d '{\"message\":\"hey!!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ < hey!!hello world > ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Operations"},{"location":"tutorials/02-parameterization/#trigger-template-parameterization","text":"The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/02-parameterization/sensor-05.yaml Send a HTTP request to the event-source. curl -d '{\"dependencyName\":\"test-dep\", \"dataKey\": \"body.message\", \"dest\": \"spec.arguments.parameters.0.value\", \"message\": \"amazing!!\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ < amazing!! > ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Trigger Template Parameterization"},{"location":"tutorials/03-trigger-sources/","text":"Trigger Sources \u00b6 A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources. Prerequisites \u00b6 The Webhook event-source is already set up. Git \u00b6 Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl -n argo-events create secret generic git-ssh --from-file=key=.ssh/<YOUR_SSH_KEY_FILE_NAME> Create a K8s secret that holds known hosts. kubectl -n argo-events create secret generic git-known-hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world workflow stored on the Argo Git project kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/sensor-git.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf S3 \u00b6 You can refer to the K8s resource stored on S3 compliant store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/sensor-minio.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf K8s Configmap \u00b6 K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/trigger-store.yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/sensor-cm.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf File & URL \u00b6 File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#trigger-sources","text":"A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#prerequisites","text":"The Webhook event-source is already set up.","title":"Prerequisites"},{"location":"tutorials/03-trigger-sources/#git","text":"Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl -n argo-events create secret generic git-ssh --from-file=key=.ssh/<YOUR_SSH_KEY_FILE_NAME> Create a K8s secret that holds known hosts. kubectl -n argo-events create secret generic git-known-hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world workflow stored on the Argo Git project kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/sensor-git.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf","title":"Git"},{"location":"tutorials/03-trigger-sources/#s3","text":"You can refer to the K8s resource stored on S3 compliant store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/sensor-minio.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf","title":"S3"},{"location":"tutorials/03-trigger-sources/#k8s-configmap","text":"K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/trigger-store.yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/03-trigger-sources/sensor-cm.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf","title":"K8s Configmap"},{"location":"tutorials/03-trigger-sources/#file-url","text":"File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"File &amp; URL"},{"location":"tutorials/04-standard-k8s-resources/","text":"Trigger Standard K8s Resources \u00b6 In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Note: You can trigger any standard Kubernetes object. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads. Prerequisites \u00b6 Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook event-source is already set up. Pod \u00b6 Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/04-standard-k8s-resources/sensor-pod.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see a pod being created. kubectl -n argo-events get po Output _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\",\"time\":\"2020-01-11T21:23:07.682961 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Deployment \u00b6 Lets create a sensor with a K8s deployment as trigger. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/04-standard-k8s-resources/sensor-deployment.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see a deployment being created. Get the corresponding pod. kubectl -n argo-events get deployments Output _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\",\"time\":\"2020-01-11T21:23:07.682961 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Trigger Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#trigger-standard-k8s-resources","text":"In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Note: You can trigger any standard Kubernetes object. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads.","title":"Trigger Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#prerequisites","text":"Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook event-source is already set up.","title":"Prerequisites"},{"location":"tutorials/04-standard-k8s-resources/#pod","text":"Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/04-standard-k8s-resources/sensor-pod.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see a pod being created. kubectl -n argo-events get po Output _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\",\"time\":\"2020-01-11T21:23:07.682961 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Pod"},{"location":"tutorials/04-standard-k8s-resources/#deployment","text":"Lets create a sensor with a K8s deployment as trigger. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/04-standard-k8s-resources/sensor-deployment.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d '{\"message\":\"ok\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Now, you should see a deployment being created. Get the corresponding pod. kubectl -n argo-events get deployments Output _________________________________________ / {\"context\":{\"type\":\"webhook\",\"specVersi \\ | on\":\"0.3\",\"source\":\"webhook\",\"e | | ventID\":\"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\",\"time\":\"2020-01-11T21:23:07.682961 | | Z\",\"dataContentType\":\"application/json\" | | ,\"subject\":\"example\"},\"data\":\"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\"} / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Deployment"},{"location":"tutorials/05-trigger-custom-resources/","text":"Trigger Custom Resources \u00b6 Take a look at Build Your Own Trigger to customize the sensor.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#trigger-custom-resources","text":"Take a look at Build Your Own Trigger to customize the sensor.","title":"Trigger Custom Resources"},{"location":"tutorials/06-trigger-conditions/","text":"Trigger Conditions \u00b6 In the previous sections, you have been dealing with just a single dependency. But, in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through conditions . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. conditions can modify that behavior. Prerequisite \u00b6 Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 . Conditions \u00b6 Consider a scenario where you have a Webhook and Minio event-source, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook event-source, but, another workflow if it receives an event from the Minio event-source. Create the webhook event-source and event-source. The event-source listens to HTTP requests on port 12000 kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/webhook-event-source.yaml Create the minio event-source. The event-source listens to events of type PUT and DELETE for objects in bucket test . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/minio-event-source.yaml Make sure there are no errors in any of the event-sources. Let's create the sensor. If you take a closer look at the trigger templates, you will notice that it contains a field named conditions , which is a boolean expression contains dependency names. So, as soon as the expression is resolved as true, the corresponding trigger will be executed. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/sensor-01.yaml Send a HTTP request to Webhook event-source, curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follows, ______ < test > ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to use conditions . Lets update the sensor with a trigger that waits for both dependencies to resolve. This is the normal sensor behavior if conditions is not defined. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/sensor-02.yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ < this is my first webhook test > ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Trigger Conditions"},{"location":"tutorials/06-trigger-conditions/#trigger-conditions","text":"In the previous sections, you have been dealing with just a single dependency. But, in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through conditions . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. conditions can modify that behavior.","title":"Trigger Conditions"},{"location":"tutorials/06-trigger-conditions/#prerequisite","text":"Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 .","title":"Prerequisite"},{"location":"tutorials/06-trigger-conditions/#conditions","text":"Consider a scenario where you have a Webhook and Minio event-source, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook event-source, but, another workflow if it receives an event from the Minio event-source. Create the webhook event-source and event-source. The event-source listens to HTTP requests on port 12000 kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/webhook-event-source.yaml Create the minio event-source. The event-source listens to events of type PUT and DELETE for objects in bucket test . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/minio-event-source.yaml Make sure there are no errors in any of the event-sources. Let's create the sensor. If you take a closer look at the trigger templates, you will notice that it contains a field named conditions , which is a boolean expression contains dependency names. So, as soon as the expression is resolved as true, the corresponding trigger will be executed. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/sensor-01.yaml Send a HTTP request to Webhook event-source, curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follows, ______ < test > ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to use conditions . Lets update the sensor with a trigger that waits for both dependencies to resolve. This is the normal sensor behavior if conditions is not defined. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/06-trigger-conditions/sensor-02.yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ < this is my first webhook test > ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Conditions"},{"location":"tutorials/07-filters/","text":"Filters \u00b6 In the previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter Prerequisite \u00b6 Webhook event-source must be set up. Data Filter \u00b6 Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook event-source has payload structure as, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"header\": {}, \"body\": {}, } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook event-source with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data: - path: path_within_event_data type: types_of_the_data value: - list_of_possible_values Comparator \u00b6 The data filter offers comparator \u201c>=\u201d, \u201c>\u201d, \u201c=\u201d, \u201c!=\u201d, \u201c<\u201d, or \u201c<=\u201d. e.g., filters: data: - path: body.value type: number comparator: \">\" value: - \"50.0\" Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/07-filters/sensor-data-filters.yaml Send a HTTP request to event-source curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to event-source curl -d '{\"message\":\"hello\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Watch for a workflow with name data-workflow-xxxx . Context Filter \u00b6 Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook event-source to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/07-filters/sensor-context-filter.yaml Send a HTTP request to event-source curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source . Time Filter \u00b6 You can also use time filter, which is applied on event time. It filters out events that occur outside the specified time range, so it is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time in HH:MM:SS format in UTC. If stop is smaller than start , the stop time is treated as next day of start . Note that start is inclusive while stop is exclusive. The diagrams below illustlate these behavior. An example of time filter is available under examples/sensors . if start < stop : event time must be in [start, stop) 00:00:00 00:00:00 00:00:00 \u2503 start stop \u2503 start stop \u2503 \u2500\u2538\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2538\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2538\u2500 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f if stop < start : event time must be in [start, stop@Next day) (this is equivalent to: event time must be in [00:00:00, stop) || [start, 00:00:00@Next day) ) 00:00:00 00:00:00 00:00:00 \u2503 stop start \u2503 stop start \u2503 \u2500\u2538\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2538\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2538\u2500 \u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500","title":"Filters"},{"location":"tutorials/07-filters/#filters","text":"In the previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter","title":"Filters"},{"location":"tutorials/07-filters/#prerequisite","text":"Webhook event-source must be set up.","title":"Prerequisite"},{"location":"tutorials/07-filters/#data-filter","text":"Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook event-source has payload structure as, { \"context\": { \"type\": \"type_of_event_source\", \"specversion\": \"cloud_events_version\", \"source\": \"name_of_the_event_source\", \"id\": \"unique_event_id\", \"time\": \"event_time\", \"datacontenttype\": \"type_of_data\", \"subject\": \"name_of_the_configuration_within_event_source\" }, \"data\": { \"header\": {}, \"body\": {}, } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook event-source with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data: - path: path_within_event_data type: types_of_the_data value: - list_of_possible_values","title":"Data Filter"},{"location":"tutorials/07-filters/#comparator","text":"The data filter offers comparator \u201c>=\u201d, \u201c>\u201d, \u201c=\u201d, \u201c!=\u201d, \u201c<\u201d, or \u201c<=\u201d. e.g., filters: data: - path: body.value type: number comparator: \">\" value: - \"50.0\" Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/07-filters/sensor-data-filters.yaml Send a HTTP request to event-source curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to event-source curl -d '{\"message\":\"hello\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example Watch for a workflow with name data-workflow-xxxx .","title":"Comparator"},{"location":"tutorials/07-filters/#context-filter","text":"Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook event-source to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/tutorials/07-filters/sensor-context-filter.yaml Send a HTTP request to event-source curl -d '{\"message\":\"this is my first webhook\"}' -H \"Content-Type: application/json\" -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source .","title":"Context Filter"},{"location":"tutorials/07-filters/#time-filter","text":"You can also use time filter, which is applied on event time. It filters out events that occur outside the specified time range, so it is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time in HH:MM:SS format in UTC. If stop is smaller than start , the stop time is treated as next day of start . Note that start is inclusive while stop is exclusive. The diagrams below illustlate these behavior. An example of time filter is available under examples/sensors . if start < stop : event time must be in [start, stop) 00:00:00 00:00:00 00:00:00 \u2503 start stop \u2503 start stop \u2503 \u2500\u2538\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2538\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2538\u2500 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f if stop < start : event time must be in [start, stop@Next day) (this is equivalent to: event time must be in [00:00:00, stop) || [start, 00:00:00@Next day) ) 00:00:00 00:00:00 00:00:00 \u2503 stop start \u2503 stop start \u2503 \u2500\u2538\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2538\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2538\u2500 \u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500 OK \u2500\u2500\u2500","title":"Time Filter"},{"location":"tutorials/08-policy/","text":"Policy \u00b6 A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc. Resource Labels Policy \u00b6 This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here . Status Policy \u00b6 For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Policy"},{"location":"tutorials/08-policy/#policy","text":"A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc.","title":"Policy"},{"location":"tutorials/08-policy/#resource-labels-policy","text":"This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here .","title":"Resource Labels Policy"},{"location":"tutorials/08-policy/#status-policy","text":"For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Status Policy"}]}